* 
* ==> Audit <==
* |----------------|---------------------------|----------|---------|---------|-------------------------------|-------------------------------|
|    Command     |           Args            | Profile  |  User   | Version |          Start Time           |           End Time            |
|----------------|---------------------------|----------|---------|---------|-------------------------------|-------------------------------|
| update-check   |                           | minikube | mikexie | v1.25.2 | Tue, 25 Oct 2022 13:09:03 CST | Tue, 25 Oct 2022 13:09:03 CST |
| update-check   |                           | minikube | mikexie | v1.25.2 | Thu, 27 Oct 2022 00:30:56 CST | Thu, 27 Oct 2022 00:30:57 CST |
| update-check   |                           | minikube | mikexie | v1.25.2 | Thu, 17 Nov 2022 14:57:57 CST | Thu, 17 Nov 2022 14:57:58 CST |
| update-check   |                           | minikube | mikexie | v1.25.2 | Sat, 19 Nov 2022 16:59:41 CST | Sat, 19 Nov 2022 16:59:41 CST |
| update-check   |                           | minikube | mikexie | v1.25.2 | Tue, 20 Dec 2022 12:26:24 CST | Tue, 20 Dec 2022 12:26:24 CST |
| start          |                           | minikube | mikexie | v1.26.1 | 06 Jan 23 00:34 CST           |                               |
| kubectl        | -- get po -A              | minikube | mikexie | v1.26.1 | 06 Jan 23 00:54 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 06 Jan 23 00:54 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 06 Jan 23 01:09 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 06 Jan 23 01:11 CST           | 06 Jan 23 01:13 CST           |
| update-check   |                           | minikube | mikexie | v1.25.2 | Mon, 06 Feb 2023 23:57:04 CST | Mon, 06 Feb 2023 23:57:04 CST |
| update-check   |                           | minikube | mikexie | v1.25.2 | Sat, 11 Feb 2023 23:45:08 CST | Sat, 11 Feb 2023 23:45:10 CST |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 00:03 CST           | 12 Feb 23 00:08 CST           |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 13:35 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 14:38 CST           |                               |
| update-context |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 14:50 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 14:51 CST           |                               |
| stop           |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 15:21 CST           | 12 Feb 23 15:21 CST           |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 15:39 CST           |                               |
| start          | --driver=docker           | minikube | mikexie | v1.26.1 | 12 Feb 23 15:54 CST           |                               |
| start          | --driver=docker           | minikube | mikexie | v1.26.1 | 12 Feb 23 15:57 CST           | 12 Feb 23 16:01 CST           |
| stop           |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 18:26 CST           | 12 Feb 23 18:26 CST           |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 18:27 CST           | 12 Feb 23 18:29 CST           |
| stop           |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 19:28 CST           | 12 Feb 23 19:28 CST           |
| start          |                           | minikube | mikexie | v1.26.1 | 12 Feb 23 19:45 CST           | 12 Feb 23 19:48 CST           |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 00:29 CST           | 19 Feb 23 00:29 CST           |
| addons         | enbale ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 00:30 CST           | 19 Feb 23 00:30 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 00:30 CST           |                               |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 00:43 CST           |                               |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 01:21 CST           |                               |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 11:12 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 11:21 CST           | 19 Feb 23 11:21 CST           |
| addons         | enable ambassador         | minikube | mikexie | v1.26.1 | 19 Feb 23 11:22 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 11:24 CST           | 19 Feb 23 11:24 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 11:24 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 11:58 CST           | 19 Feb 23 11:58 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 11:58 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 19 Feb 23 14:12 CST           | 19 Feb 23 14:15 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 14:16 CST           |                               |
| start          |                           | minikube | mikexie | v1.26.1 | 19 Feb 23 15:08 CST           | 19 Feb 23 15:11 CST           |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:12 CST           | 19 Feb 23 15:12 CST           |
| start          |                           | minikube | mikexie | v1.26.1 | 19 Feb 23 15:12 CST           | 19 Feb 23 15:16 CST           |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:18 CST           | 19 Feb 23 15:18 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 15:18 CST           |                               |
| addons         | enable ingress-ignix      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:48 CST           |                               |
| addons         | enable ingress-nginx      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:53 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:56 CST           | 19 Feb 23 15:56 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 16:20 CST           |                               |
| addons         | enable registry           | minikube | mikexie | v1.26.1 | 19 Feb 23 16:30 CST           |                               |
| addons         | enable ingress-dns        | minikube | mikexie | v1.26.1 | 19 Feb 23 16:35 CST           | 19 Feb 23 16:35 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 16:35 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 16:41 CST           | 19 Feb 23 16:41 CST           |
| start          | --driver docker -p docker | docker   | mikexie | v1.26.1 | 19 Feb 23 16:54 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 17:17 CST           | 19 Feb 23 17:17 CST           |
| addons         | disable ingress-dns       | minikube | mikexie | v1.26.1 | 19 Feb 23 17:17 CST           | 19 Feb 23 17:17 CST           |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 17:18 CST           |                               |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 17:26 CST           |                               |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 17:29 CST           |                               |
| addons         | enable ingress            | minikube | mikexie | v1.26.1 | 19 Feb 23 23:13 CST           |                               |
| addons         | list                      | minikube | mikexie | v1.26.1 | 19 Feb 23 23:24 CST           | 19 Feb 23 23:24 CST           |
|----------------|---------------------------|----------|---------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/02/19 16:54:37
Running on machine: Mikes-MBP
Binary: Built with gc go1.18.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0219 16:54:37.882726   67048 out.go:296] Setting OutFile to fd 1 ...
I0219 16:54:37.883080   67048 out.go:348] isatty.IsTerminal(1) = true
I0219 16:54:37.883086   67048 out.go:309] Setting ErrFile to fd 2...
I0219 16:54:37.883090   67048 out.go:348] isatty.IsTerminal(2) = true
I0219 16:54:37.908472   67048 out.go:177] [31m╭──────────────────────────────────────────────────────────────────────────────────────────────────────────╮[0m
[31m│[0m                                                                                                          [31m│[0m
[31m│[0m    You are trying to run the amd64 binary on an M1 system.                                               [31m│[0m
[31m│[0m    Please consider running the darwin/arm64 binary instead.                                              [31m│[0m
[31m│[0m    Download at https://github.com/kubernetes/minikube/releases/download/v1.26.1/minikube-darwin-arm64    [31m│[0m
[31m│[0m                                                                                                          [31m│[0m
[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m
I0219 16:54:37.928831   67048 root.go:333] Updating PATH: /Users/mikexie/.minikube/bin
I0219 16:54:37.931515   67048 out.go:303] Setting JSON to false
I0219 16:54:37.971331   67048 start.go:115] hostinfo: {"hostname":"Mikes-MBP.lan","uptime":8553008,"bootTime":1668243869,"procs":514,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.2","kernelVersion":"21.3.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"dba72b4f-1677-5036-bfda-458b76b6aa28"}
W0219 16:54:37.971499   67048 start.go:123] gopshost.Virtualization returned error: not implemented yet
I0219 16:54:37.991466   67048 out.go:177] 😄  [docker] minikube v1.26.1 on Darwin 12.2
I0219 16:54:38.031362   67048 notify.go:193] Checking for updates...
I0219 16:54:38.031935   67048 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0219 16:54:38.032666   67048 driver.go:365] Setting default libvirt URI to qemu:///system
I0219 16:54:38.247592   67048 docker.go:137] docker version: linux-20.10.12
I0219 16:54:38.247731   67048 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0219 16:54:38.594556   67048 info.go:265] docker info: {ID:CTCL:4RAO:P5RY:ARFY:URFT:O6BG:J3YV:MFMN:UKU5:PBGR:6MIS:ZCYV Containers:4 ContainersRunning:1 ContainersPaused:0 ContainersStopped:3 Images:27 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:78 OomKillDisable:false NGoroutines:68 SystemTime:2023-02-19 08:54:38.359878005 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:2085158912 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0219 16:54:38.615981   67048 out.go:177] ✨  Using the docker driver based on user configuration
I0219 16:54:38.654659   67048 start.go:284] selected driver: docker
I0219 16:54:38.654687   67048 start.go:808] validating driver "docker" against <nil>
I0219 16:54:38.654722   67048 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0219 16:54:38.654882   67048 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0219 16:54:38.837393   67048 info.go:265] docker info: {ID:CTCL:4RAO:P5RY:ARFY:URFT:O6BG:J3YV:MFMN:UKU5:PBGR:6MIS:ZCYV Containers:4 ContainersRunning:1 ContainersPaused:0 ContainersStopped:3 Images:27 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:78 OomKillDisable:false NGoroutines:68 SystemTime:2023-02-19 08:54:38.76662588 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:2085158912 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0219 16:54:38.837990   67048 start_flags.go:296] no existing cluster config was found, will generate one from the flags 
I0219 16:54:38.846844   67048 start_flags.go:377] Using suggested 1988MB memory alloc based on sys=16384MB, container=1988MB
I0219 16:54:38.847345   67048 start_flags.go:835] Wait components to verify : map[apiserver:true system_pods:true]
I0219 16:54:38.866451   67048 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0219 16:54:38.885868   67048 cni.go:95] Creating CNI manager for ""
I0219 16:54:38.885882   67048 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0219 16:54:38.885895   67048 start_flags.go:310] config:
{Name:docker KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:1988 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:docker Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0219 16:54:38.905069   67048 out.go:177] 👍  Starting control plane node docker in cluster docker
I0219 16:54:38.959105   67048 cache.go:120] Beginning downloading kic base image for docker with docker
I0219 16:54:38.994038   67048 out.go:177] 🚜  Pulling base image ...
I0219 16:54:39.031962   67048 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0219 16:54:39.032230   67048 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I0219 16:54:39.176070   67048 cache.go:147] Downloading gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local cache
I0219 16:54:39.177510   67048 image.go:59] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local cache directory
I0219 16:54:39.177757   67048 image.go:119] Writing gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local cache
I0219 16:54:39.228663   67048 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.24.3/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4
I0219 16:54:39.228785   67048 cache.go:57] Caching tarball of preloaded images
I0219 16:54:39.229010   67048 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0219 16:54:39.249713   67048 out.go:177] 💾  Downloading Kubernetes v1.24.3 preload ...
I0219 16:54:39.269371   67048 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4 ...
I0219 16:54:39.654431   67048 download.go:101] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.24.3/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4?checksum=md5:6e57d4bdb605594270eaf0a829e4a120 -> /Users/mikexie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4
I0219 16:55:08.402768   67048 cache.go:161] Loading gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 from local cache
I0219 16:55:08.403290   67048 cache.go:170] Downloading gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local daemon
I0219 16:55:08.405316   67048 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I0219 16:55:08.552705   67048 image.go:243] Writing gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local daemon
I0219 16:55:13.210817   67048 preload.go:249] saving checksum for preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4 ...
I0219 16:55:13.210950   67048 preload.go:256] verifying checksumm of /Users/mikexie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4 ...
I0219 16:55:14.092505   67048 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.3 on docker
I0219 16:55:14.092627   67048 profile.go:148] Saving config to /Users/mikexie/.minikube/profiles/docker/config.json ...
I0219 16:55:14.092646   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/config.json: {Name:mke1f8e41001e53f3f748f525ada30577b2d5178 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 16:55:37.638429   67048 cache.go:182] failed to download gcr.io/k8s-minikube/kicbase:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8, will try fallback image if available: getting remote image: Get "https://gcr.io/v2/": dial tcp 64.233.189.82:443: i/o timeout
I0219 16:55:37.638923   67048 image.go:75] Checking for docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I0219 16:55:37.869706   67048 cache.go:147] Downloading docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local cache
I0219 16:55:37.870349   67048 image.go:59] Checking for docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local cache directory
I0219 16:55:37.870408   67048 image.go:119] Writing docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local cache
I0219 16:58:01.129632   67048 cache.go:150] successfully saved docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 as a tarball
I0219 16:58:01.129662   67048 cache.go:161] Loading docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 from local cache
I0219 16:58:25.744881   67048 cache.go:164] successfully loaded docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 from cached tarball
I0219 16:58:25.744908   67048 cache.go:170] Downloading docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local daemon
I0219 16:58:25.745492   67048 image.go:75] Checking for docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 in local docker daemon
I0219 16:58:26.003310   67048 image.go:243] Writing docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 to local daemon
I0219 17:02:04.355249   67048 cache.go:173] successfully downloaded docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8
W0219 17:02:04.356485   67048 out.go:239] ❗  minikube was unable to download gcr.io/k8s-minikube/kicbase:v0.0.33, but successfully downloaded docker.io/kicbase/stable:v0.0.33 as a fallback image
I0219 17:02:04.357541   67048 cache.go:208] Successfully downloaded all kic artifacts
I0219 17:02:04.359214   67048 start.go:371] acquiring machines lock for docker: {Name:mkbd886b110db3fed39ae4ee0006ecafe8f8e4a4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0219 17:02:04.377736   67048 start.go:375] acquired machines lock for "docker" in 18.462542ms
I0219 17:02:04.380028   67048 start.go:92] Provisioning new machine with config: &{Name:docker KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:1988 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:docker Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name: IP: Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0219 17:02:04.380262   67048 start.go:132] createHost starting for "" (driver="docker")
I0219 17:02:04.427269   67048 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=1988MB) ...
I0219 17:02:04.431669   67048 start.go:166] libmachine.API.Create for "docker" (driver="docker")
I0219 17:02:04.431758   67048 client.go:168] LocalClient.Create starting
I0219 17:02:04.433700   67048 main.go:134] libmachine: Reading certificate data from /Users/mikexie/.minikube/certs/ca.pem
I0219 17:02:04.434149   67048 main.go:134] libmachine: Decoding PEM data...
I0219 17:02:04.434374   67048 main.go:134] libmachine: Parsing certificate...
I0219 17:02:04.437046   67048 main.go:134] libmachine: Reading certificate data from /Users/mikexie/.minikube/certs/cert.pem
I0219 17:02:04.437317   67048 main.go:134] libmachine: Decoding PEM data...
I0219 17:02:04.437345   67048 main.go:134] libmachine: Parsing certificate...
I0219 17:02:04.440395   67048 cli_runner.go:164] Run: docker network inspect docker --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0219 17:02:04.589848   67048 cli_runner.go:211] docker network inspect docker --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0219 17:02:04.590477   67048 network_create.go:272] running [docker network inspect docker] to gather additional debugging logs...
I0219 17:02:04.590513   67048 cli_runner.go:164] Run: docker network inspect docker
W0219 17:02:04.713612   67048 cli_runner.go:211] docker network inspect docker returned with exit code 1
I0219 17:02:04.713639   67048 network_create.go:275] error running [docker network inspect docker]: docker network inspect docker: exit status 1
stdout:
[]

stderr:
Error: No such network: docker
I0219 17:02:04.713652   67048 network_create.go:277] output of [docker network inspect docker]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: docker

** /stderr **
I0219 17:02:04.714367   67048 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0219 17:02:04.838003   67048 network.go:288] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc0008a4008] misses:0}
I0219 17:02:04.838205   67048 network.go:235] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I0219 17:02:04.838233   67048 network_create.go:115] attempt to create docker network docker 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0219 17:02:04.838341   67048 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=docker docker
W0219 17:02:04.992156   67048 cli_runner.go:211] docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=docker docker returned with exit code 1
W0219 17:02:04.992214   67048 network_create.go:107] failed to create docker network docker 192.168.49.0/24, will retry: subnet is taken
I0219 17:02:04.992541   67048 network.go:279] skipping subnet 192.168.49.0 that has unexpired reservation: &{mu:{state:0 sema:0} read:{v:{m:map[192.168.49.0:0xc0008a4008] amended:false}} dirty:map[] misses:0}
I0219 17:02:04.992558   67048 network.go:238] skipping subnet 192.168.49.0/24 that is reserved: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I0219 17:02:04.992717   67048 network.go:288] reserving subnet 192.168.58.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[192.168.49.0:0xc0008a4008] amended:true}} dirty:map[192.168.49.0:0xc0008a4008 192.168.58.0:0xc00000eeb0] misses:0}
I0219 17:02:04.992730   67048 network.go:235] using free private subnet 192.168.58.0/24: &{IP:192.168.58.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.58.0/24 Gateway:192.168.58.1 ClientMin:192.168.58.2 ClientMax:192.168.58.254 Broadcast:192.168.58.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I0219 17:02:04.992735   67048 network_create.go:115] attempt to create docker network docker 192.168.58.0/24 with gateway 192.168.58.1 and MTU of 1500 ...
I0219 17:02:04.992821   67048 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.58.0/24 --gateway=192.168.58.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=docker docker
I0219 17:02:05.167855   67048 network_create.go:99] docker network docker 192.168.58.0/24 created
I0219 17:02:05.168766   67048 kic.go:106] calculated static IP "192.168.58.2" for the "docker" container
I0219 17:02:05.168988   67048 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0219 17:02:05.303346   67048 cli_runner.go:164] Run: docker volume create docker --label name.minikube.sigs.k8s.io=docker --label created_by.minikube.sigs.k8s.io=true
I0219 17:02:05.433409   67048 oci.go:103] Successfully created a docker volume docker
I0219 17:02:05.433654   67048 cli_runner.go:164] Run: docker run --rm --name docker-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=docker --entrypoint /usr/bin/test -v docker:/var docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 -d /var/lib
I0219 17:02:06.403321   67048 oci.go:107] Successfully prepared a docker volume docker
I0219 17:02:06.403478   67048 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0219 17:02:06.404631   67048 kic.go:179] Starting extracting preloaded images to volume ...
I0219 17:02:06.404875   67048 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/mikexie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v docker:/extractDir docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 -I lz4 -xf /preloaded.tar -C /extractDir
I0219 17:02:18.224855   67048 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/mikexie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v docker:/extractDir docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 -I lz4 -xf /preloaded.tar -C /extractDir: (11.818367375s)
I0219 17:02:18.225689   67048 kic.go:188] duration metric: took 11.821334 seconds to extract preloaded images to volume
I0219 17:02:18.226188   67048 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0219 17:02:18.672949   67048 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname docker --name docker --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=docker --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=docker --network docker --ip 192.168.58.2 --volume docker:/var --security-opt apparmor=unconfined --memory=1988mb --memory-swap=1988mb --cpus=2 -e container=docker --expose 8443 --publish=8443 --publish=22 --publish=2376 --publish=5000 --publish=32443 docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8
I0219 17:02:19.241676   67048 cli_runner.go:164] Run: docker container inspect docker --format={{.State.Running}}
I0219 17:02:19.375438   67048 cli_runner.go:164] Run: docker container inspect docker --format={{.State.Status}}
I0219 17:02:19.499804   67048 cli_runner.go:164] Run: docker exec docker stat /var/lib/dpkg/alternatives/iptables
I0219 17:02:19.702977   67048 oci.go:144] the created container "docker" has a running status.
I0219 17:02:19.703398   67048 kic.go:210] Creating ssh key for kic: /Users/mikexie/.minikube/machines/docker/id_rsa...
I0219 17:02:20.374156   67048 kic_runner.go:191] docker (temp): /Users/mikexie/.minikube/machines/docker/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0219 17:02:20.575886   67048 cli_runner.go:164] Run: docker container inspect docker --format={{.State.Status}}
I0219 17:02:20.692955   67048 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0219 17:02:20.692973   67048 kic_runner.go:114] Args: [docker exec --privileged docker chown docker:docker /home/docker/.ssh/authorized_keys]
I0219 17:02:20.884635   67048 cli_runner.go:164] Run: docker container inspect docker --format={{.State.Status}}
I0219 17:02:21.003736   67048 machine.go:88] provisioning docker machine ...
I0219 17:02:21.004737   67048 ubuntu.go:169] provisioning hostname "docker"
I0219 17:02:21.004855   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:21.125302   67048 main.go:134] libmachine: Using SSH client type: native
I0219 17:02:21.125848   67048 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 56362 <nil> <nil>}
I0219 17:02:21.125862   67048 main.go:134] libmachine: About to run SSH command:
sudo hostname docker && echo "docker" | sudo tee /etc/hostname
I0219 17:02:21.268088   67048 main.go:134] libmachine: SSH cmd err, output: <nil>: docker

I0219 17:02:21.268579   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:21.395549   67048 main.go:134] libmachine: Using SSH client type: native
I0219 17:02:21.395773   67048 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 56362 <nil> <nil>}
I0219 17:02:21.395861   67048 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sdocker' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 docker/g' /etc/hosts;
			else 
				echo '127.0.1.1 docker' | sudo tee -a /etc/hosts; 
			fi
		fi
I0219 17:02:21.513419   67048 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0219 17:02:21.514774   67048 ubuntu.go:175] set auth options {CertDir:/Users/mikexie/.minikube CaCertPath:/Users/mikexie/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/mikexie/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/mikexie/.minikube/machines/server.pem ServerKeyPath:/Users/mikexie/.minikube/machines/server-key.pem ClientKeyPath:/Users/mikexie/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/mikexie/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/mikexie/.minikube}
I0219 17:02:21.514803   67048 ubuntu.go:177] setting up certificates
I0219 17:02:21.514813   67048 provision.go:83] configureAuth start
I0219 17:02:21.514901   67048 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" docker
I0219 17:02:21.632086   67048 provision.go:138] copyHostCerts
I0219 17:02:21.632186   67048 exec_runner.go:144] found /Users/mikexie/.minikube/ca.pem, removing ...
I0219 17:02:21.632193   67048 exec_runner.go:207] rm: /Users/mikexie/.minikube/ca.pem
I0219 17:02:21.632988   67048 exec_runner.go:151] cp: /Users/mikexie/.minikube/certs/ca.pem --> /Users/mikexie/.minikube/ca.pem (1078 bytes)
I0219 17:02:21.633302   67048 exec_runner.go:144] found /Users/mikexie/.minikube/cert.pem, removing ...
I0219 17:02:21.633307   67048 exec_runner.go:207] rm: /Users/mikexie/.minikube/cert.pem
I0219 17:02:21.633363   67048 exec_runner.go:151] cp: /Users/mikexie/.minikube/certs/cert.pem --> /Users/mikexie/.minikube/cert.pem (1123 bytes)
I0219 17:02:21.633584   67048 exec_runner.go:144] found /Users/mikexie/.minikube/key.pem, removing ...
I0219 17:02:21.633586   67048 exec_runner.go:207] rm: /Users/mikexie/.minikube/key.pem
I0219 17:02:21.633635   67048 exec_runner.go:151] cp: /Users/mikexie/.minikube/certs/key.pem --> /Users/mikexie/.minikube/key.pem (1675 bytes)
I0219 17:02:21.633817   67048 provision.go:112] generating server cert: /Users/mikexie/.minikube/machines/server.pem ca-key=/Users/mikexie/.minikube/certs/ca.pem private-key=/Users/mikexie/.minikube/certs/ca-key.pem org=mikexie.docker san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube docker]
I0219 17:02:21.808179   67048 provision.go:172] copyRemoteCerts
I0219 17:02:21.808391   67048 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0219 17:02:21.808436   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:21.925744   67048 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56362 SSHKeyPath:/Users/mikexie/.minikube/machines/docker/id_rsa Username:docker}
I0219 17:02:22.008942   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0219 17:02:22.028102   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I0219 17:02:22.042329   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0219 17:02:22.056340   67048 provision.go:86] duration metric: configureAuth took 541.514916ms
I0219 17:02:22.056488   67048 ubuntu.go:193] setting minikube options for container-runtime
I0219 17:02:22.056993   67048 config.go:180] Loaded profile config "docker": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.3
I0219 17:02:22.057059   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:22.177865   67048 main.go:134] libmachine: Using SSH client type: native
I0219 17:02:22.178044   67048 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 56362 <nil> <nil>}
I0219 17:02:22.178073   67048 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0219 17:02:22.293166   67048 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0219 17:02:22.293670   67048 ubuntu.go:71] root file system type: overlay
I0219 17:02:22.294303   67048 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0219 17:02:22.294404   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:22.437720   67048 main.go:134] libmachine: Using SSH client type: native
I0219 17:02:22.437927   67048 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 56362 <nil> <nil>}
I0219 17:02:22.437983   67048 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0219 17:02:22.563532   67048 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0219 17:02:22.564003   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:22.692725   67048 main.go:134] libmachine: Using SSH client type: native
I0219 17:02:22.692918   67048 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 56362 <nil> <nil>}
I0219 17:02:22.692930   67048 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0219 17:02:23.246779   67048 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:00:44.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2023-02-19 09:02:22.560512012 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0219 17:02:23.247535   67048 machine.go:91] provisioned docker machine in 2.243774792s
I0219 17:02:23.247653   67048 client.go:171] LocalClient.Create took 18.814975375s
I0219 17:02:23.248003   67048 start.go:174] duration metric: libmachine.API.Create for "docker" took 18.816287625s
I0219 17:02:23.248024   67048 start.go:307] post-start starting for "docker" (driver="docker")
I0219 17:02:23.248031   67048 start.go:335] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0219 17:02:23.248216   67048 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0219 17:02:23.248300   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:23.379277   67048 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56362 SSHKeyPath:/Users/mikexie/.minikube/machines/docker/id_rsa Username:docker}
I0219 17:02:23.463594   67048 ssh_runner.go:195] Run: cat /etc/os-release
I0219 17:02:23.467332   67048 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0219 17:02:23.467345   67048 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0219 17:02:23.467351   67048 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0219 17:02:23.467357   67048 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I0219 17:02:23.467593   67048 filesync.go:126] Scanning /Users/mikexie/.minikube/addons for local assets ...
I0219 17:02:23.467765   67048 filesync.go:126] Scanning /Users/mikexie/.minikube/files for local assets ...
I0219 17:02:23.467794   67048 start.go:310] post-start completed in 219.76575ms
I0219 17:02:23.468246   67048 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" docker
I0219 17:02:23.588805   67048 profile.go:148] Saving config to /Users/mikexie/.minikube/profiles/docker/config.json ...
I0219 17:02:23.589379   67048 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0219 17:02:23.589423   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:23.707271   67048 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56362 SSHKeyPath:/Users/mikexie/.minikube/machines/docker/id_rsa Username:docker}
I0219 17:02:23.790947   67048 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0219 17:02:23.796062   67048 start.go:135] duration metric: createHost completed in 19.415734917s
I0219 17:02:23.796119   67048 start.go:82] releasing machines lock for "docker", held for 19.417751917s
I0219 17:02:23.796735   67048 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" docker
I0219 17:02:23.918162   67048 ssh_runner.go:195] Run: systemctl --version
I0219 17:02:23.918245   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:23.918380   67048 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0219 17:02:23.922620   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" docker
I0219 17:02:24.045562   67048 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56362 SSHKeyPath:/Users/mikexie/.minikube/machines/docker/id_rsa Username:docker}
I0219 17:02:24.045785   67048 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56362 SSHKeyPath:/Users/mikexie/.minikube/machines/docker/id_rsa Username:docker}
I0219 17:02:24.127232   67048 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0219 17:02:26.181427   67048 ssh_runner.go:235] Completed: sudo systemctl cat docker.service: (2.053869208s)
I0219 17:02:26.181427   67048 ssh_runner.go:235] Completed: curl -sS -m 2 https://k8s.gcr.io/: (2.2630025s)
W0219 17:02:26.182304   67048 start.go:734] [curl -sS -m 2 https://k8s.gcr.io/] failed: curl -sS -m 2 https://k8s.gcr.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Connection timed out after 2001 milliseconds
I0219 17:02:26.182692   67048 cruntime.go:273] skipping containerd shutdown because we are bound to it
W0219 17:02:26.183246   67048 out.go:239] ❗  This container is having trouble accessing https://k8s.gcr.io
W0219 17:02:26.183520   67048 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0219 17:02:26.183605   67048 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0219 17:02:26.211021   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0219 17:02:26.228291   67048 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0219 17:02:26.300509   67048 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0219 17:02:26.358762   67048 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 17:02:26.414393   67048 ssh_runner.go:195] Run: sudo systemctl restart docker
I0219 17:02:26.583604   67048 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0219 17:02:26.641096   67048 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 17:02:26.699665   67048 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0219 17:02:26.710586   67048 start.go:450] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0219 17:02:26.717618   67048 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0219 17:02:26.723884   67048 start.go:471] Will wait 60s for crictl version
I0219 17:02:26.724028   67048 ssh_runner.go:195] Run: sudo crictl version
I0219 17:02:26.928404   67048 start.go:480] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I0219 17:02:26.929374   67048 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0219 17:02:27.029118   67048 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0219 17:02:27.103858   67048 out.go:204] 🐳  Preparing Kubernetes v1.24.3 on Docker 20.10.17 ...
I0219 17:02:27.104362   67048 cli_runner.go:164] Run: docker exec -t docker dig +short host.docker.internal
I0219 17:02:27.348889   67048 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0219 17:02:27.349908   67048 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0219 17:02:27.353761   67048 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.2	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0219 17:02:27.364605   67048 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" docker
I0219 17:02:27.484650   67048 preload.go:132] Checking if preload exists for k8s version v1.24.3 and runtime docker
I0219 17:02:27.484729   67048 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0219 17:02:27.513192   67048 docker.go:611] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0219 17:02:27.513383   67048 docker.go:542] Images already preloaded, skipping extraction
I0219 17:02:27.513870   67048 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0219 17:02:27.541389   67048 docker.go:611] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.24.3
k8s.gcr.io/kube-scheduler:v1.24.3
k8s.gcr.io/kube-controller-manager:v1.24.3
k8s.gcr.io/kube-proxy:v1.24.3
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0219 17:02:27.541702   67048 cache_images.go:84] Images are preloaded, skipping loading
I0219 17:02:27.542258   67048 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0219 17:02:27.742395   67048 cni.go:95] Creating CNI manager for ""
I0219 17:02:27.742417   67048 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0219 17:02:27.742996   67048 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0219 17:02:27.744103   67048 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.24.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:docker NodeName:docker DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0219 17:02:27.744471   67048 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "docker"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0219 17:02:27.745660   67048 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=docker --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.3 ClusterName:docker Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0219 17:02:27.745826   67048 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.3
I0219 17:02:27.755924   67048 binaries.go:44] Found k8s binaries, skipping transfer
I0219 17:02:27.756125   67048 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0219 17:02:27.764510   67048 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (468 bytes)
I0219 17:02:27.778848   67048 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0219 17:02:27.792077   67048 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2028 bytes)
I0219 17:02:27.805062   67048 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I0219 17:02:27.809452   67048 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0219 17:02:27.819781   67048 certs.go:54] Setting up /Users/mikexie/.minikube/profiles/docker for IP: 192.168.58.2
I0219 17:02:27.820022   67048 certs.go:182] skipping minikubeCA CA generation: /Users/mikexie/.minikube/ca.key
I0219 17:02:27.821094   67048 certs.go:182] skipping proxyClientCA CA generation: /Users/mikexie/.minikube/proxy-client-ca.key
I0219 17:02:27.821170   67048 certs.go:302] generating minikube-user signed cert: /Users/mikexie/.minikube/profiles/docker/client.key
I0219 17:02:27.821467   67048 crypto.go:68] Generating cert /Users/mikexie/.minikube/profiles/docker/client.crt with IP's: []
I0219 17:02:28.028901   67048 crypto.go:156] Writing cert to /Users/mikexie/.minikube/profiles/docker/client.crt ...
I0219 17:02:28.028916   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/client.crt: {Name:mk52e9cb1043768e536d8c5f62c004e23c150f3f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 17:02:28.029613   67048 crypto.go:164] Writing key to /Users/mikexie/.minikube/profiles/docker/client.key ...
I0219 17:02:28.029618   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/client.key: {Name:mk745542b03f98359a10c865a9dacccf9d588442 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 17:02:28.030503   67048 certs.go:302] generating minikube signed cert: /Users/mikexie/.minikube/profiles/docker/apiserver.key.cee25041
I0219 17:02:28.030513   67048 crypto.go:68] Generating cert /Users/mikexie/.minikube/profiles/docker/apiserver.crt.cee25041 with IP's: [192.168.58.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0219 17:02:28.163569   67048 crypto.go:156] Writing cert to /Users/mikexie/.minikube/profiles/docker/apiserver.crt.cee25041 ...
I0219 17:02:28.163582   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/apiserver.crt.cee25041: {Name:mk72e87f5dd866b03dbb47bc97178fdf50fe4bca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 17:02:28.164186   67048 crypto.go:164] Writing key to /Users/mikexie/.minikube/profiles/docker/apiserver.key.cee25041 ...
I0219 17:02:28.164191   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/apiserver.key.cee25041: {Name:mk7f167c62da3abb5bd928d6f261a46a352b691d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 17:02:28.164334   67048 certs.go:320] copying /Users/mikexie/.minikube/profiles/docker/apiserver.crt.cee25041 -> /Users/mikexie/.minikube/profiles/docker/apiserver.crt
I0219 17:02:28.164484   67048 certs.go:324] copying /Users/mikexie/.minikube/profiles/docker/apiserver.key.cee25041 -> /Users/mikexie/.minikube/profiles/docker/apiserver.key
I0219 17:02:28.164593   67048 certs.go:302] generating aggregator signed cert: /Users/mikexie/.minikube/profiles/docker/proxy-client.key
I0219 17:02:28.164606   67048 crypto.go:68] Generating cert /Users/mikexie/.minikube/profiles/docker/proxy-client.crt with IP's: []
I0219 17:02:28.439659   67048 crypto.go:156] Writing cert to /Users/mikexie/.minikube/profiles/docker/proxy-client.crt ...
I0219 17:02:28.439674   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/proxy-client.crt: {Name:mkca0e53a7a95540d165572fbe3906883c5dec41 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 17:02:28.440045   67048 crypto.go:164] Writing key to /Users/mikexie/.minikube/profiles/docker/proxy-client.key ...
I0219 17:02:28.440048   67048 lock.go:35] WriteFile acquiring /Users/mikexie/.minikube/profiles/docker/proxy-client.key: {Name:mka5a1709f564beea1e7046b1b685e6eae53bb4f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 17:02:28.440872   67048 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/ca-key.pem (1679 bytes)
I0219 17:02:28.440908   67048 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/ca.pem (1078 bytes)
I0219 17:02:28.440934   67048 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/cert.pem (1123 bytes)
I0219 17:02:28.440953   67048 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/key.pem (1675 bytes)
I0219 17:02:28.442216   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/docker/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0219 17:02:28.458144   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/docker/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0219 17:02:28.474561   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/docker/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0219 17:02:28.492314   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/docker/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0219 17:02:28.507438   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0219 17:02:28.521050   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0219 17:02:28.535978   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0219 17:02:28.550868   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0219 17:02:28.565123   67048 ssh_runner.go:362] scp /Users/mikexie/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0219 17:02:28.581662   67048 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0219 17:02:28.592651   67048 ssh_runner.go:195] Run: openssl version
I0219 17:02:28.602719   67048 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0219 17:02:28.610689   67048 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0219 17:02:28.614415   67048 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Mar 19  2022 /usr/share/ca-certificates/minikubeCA.pem
I0219 17:02:28.614456   67048 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0219 17:02:28.619465   67048 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0219 17:02:28.626244   67048 kubeadm.go:395] StartCluster: {Name:docker KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.33@sha256:73b259e144d926189cf169ae5b46bbec4e08e4e2f2bd87296054c3244f70feb8 Memory:1988 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.3 ClusterName:docker Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.24.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0219 17:02:28.626339   67048 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0219 17:02:28.654050   67048 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0219 17:02:28.661940   67048 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0219 17:02:28.669417   67048 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I0219 17:02:28.669515   67048 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0219 17:02:28.676112   67048 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0219 17:02:28.676448   67048 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0219 17:06:31.089788   67048 out.go:204]     ▪ Generating certificates and keys ...
I0219 17:06:31.147474   67048 out.go:204]     ▪ Booting up control plane ...
W0219 17:06:31.155458   67048 out.go:239] 💢  initialization failed, will try again: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.24.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [docker localhost] and IPs [192.168.58.2 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [docker localhost] and IPs [192.168.58.2 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0219 09:02:28.770817    1008 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING SystemVerification]: missing optional cgroups: blkio
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

I0219 17:06:31.156633   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm reset --cri-socket /var/run/cri-dockerd.sock --force"
I0219 17:06:31.971737   67048 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0219 17:06:31.982443   67048 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I0219 17:06:31.982523   67048 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0219 17:06:31.989932   67048 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0219 17:06:31.989989   67048 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0219 17:10:33.140800   67048 out.go:204]     ▪ Generating certificates and keys ...
I0219 17:10:33.200897   67048 out.go:204]     ▪ Booting up control plane ...
I0219 17:10:33.206825   67048 kubeadm.go:397] StartCluster complete in 8m4.580555125s
I0219 17:10:33.208070   67048 cri.go:52] listing CRI containers in root : {State:all Name:kube-apiserver Namespaces:[]}
I0219 17:10:33.208903   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-apiserver
I0219 17:10:33.236012   67048 cri.go:87] found id: ""
I0219 17:10:33.236929   67048 logs.go:274] 0 containers: []
W0219 17:10:33.236940   67048 logs.go:276] No container was found matching "kube-apiserver"
I0219 17:10:33.237239   67048 cri.go:52] listing CRI containers in root : {State:all Name:etcd Namespaces:[]}
I0219 17:10:33.237370   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=etcd
I0219 17:10:33.256835   67048 cri.go:87] found id: ""
I0219 17:10:33.256861   67048 logs.go:274] 0 containers: []
W0219 17:10:33.256877   67048 logs.go:276] No container was found matching "etcd"
I0219 17:10:33.256889   67048 cri.go:52] listing CRI containers in root : {State:all Name:coredns Namespaces:[]}
I0219 17:10:33.257127   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=coredns
I0219 17:10:33.277028   67048 cri.go:87] found id: ""
I0219 17:10:33.277045   67048 logs.go:274] 0 containers: []
W0219 17:10:33.277054   67048 logs.go:276] No container was found matching "coredns"
I0219 17:10:33.277070   67048 cri.go:52] listing CRI containers in root : {State:all Name:kube-scheduler Namespaces:[]}
I0219 17:10:33.277212   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-scheduler
I0219 17:10:33.296764   67048 cri.go:87] found id: ""
I0219 17:10:33.296778   67048 logs.go:274] 0 containers: []
W0219 17:10:33.296786   67048 logs.go:276] No container was found matching "kube-scheduler"
I0219 17:10:33.296793   67048 cri.go:52] listing CRI containers in root : {State:all Name:kube-proxy Namespaces:[]}
I0219 17:10:33.296974   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-proxy
I0219 17:10:33.318013   67048 cri.go:87] found id: ""
I0219 17:10:33.318031   67048 logs.go:274] 0 containers: []
W0219 17:10:33.318039   67048 logs.go:276] No container was found matching "kube-proxy"
I0219 17:10:33.318046   67048 cri.go:52] listing CRI containers in root : {State:all Name:kubernetes-dashboard Namespaces:[]}
I0219 17:10:33.318177   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kubernetes-dashboard
I0219 17:10:33.338081   67048 cri.go:87] found id: ""
I0219 17:10:33.338092   67048 logs.go:274] 0 containers: []
W0219 17:10:33.338099   67048 logs.go:276] No container was found matching "kubernetes-dashboard"
I0219 17:10:33.338104   67048 cri.go:52] listing CRI containers in root : {State:all Name:storage-provisioner Namespaces:[]}
I0219 17:10:33.338199   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=storage-provisioner
I0219 17:10:33.358245   67048 cri.go:87] found id: ""
I0219 17:10:33.358264   67048 logs.go:274] 0 containers: []
W0219 17:10:33.358272   67048 logs.go:276] No container was found matching "storage-provisioner"
I0219 17:10:33.358279   67048 cri.go:52] listing CRI containers in root : {State:all Name:kube-controller-manager Namespaces:[]}
I0219 17:10:33.358401   67048 ssh_runner.go:195] Run: sudo crictl ps -a --quiet --name=kube-controller-manager
I0219 17:10:33.379750   67048 cri.go:87] found id: ""
I0219 17:10:33.379767   67048 logs.go:274] 0 containers: []
W0219 17:10:33.379776   67048 logs.go:276] No container was found matching "kube-controller-manager"
I0219 17:10:33.380653   67048 logs.go:123] Gathering logs for kubelet ...
I0219 17:10:33.380674   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I0219 17:10:33.443894   67048 logs.go:123] Gathering logs for dmesg ...
I0219 17:10:33.443912   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I0219 17:10:33.457902   67048 logs.go:123] Gathering logs for describe nodes ...
I0219 17:10:33.457914   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
W0219 17:10:33.596615   67048 logs.go:130] failed describe nodes: command: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig" /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.3/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": Process exited with status 1
stdout:

stderr:
The connection to the server localhost:8443 was refused - did you specify the right host or port?
 output: 
** stderr ** 
The connection to the server localhost:8443 was refused - did you specify the right host or port?

** /stderr **
I0219 17:10:33.596653   67048 logs.go:123] Gathering logs for Docker ...
I0219 17:10:33.596666   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -n 400"
I0219 17:10:33.644698   67048 logs.go:123] Gathering logs for container status ...
I0219 17:10:33.644711   67048 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
W0219 17:10:33.672502   67048 out.go:369] Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.24.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0219 09:06:32.018342    2456 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING SystemVerification]: missing optional cgroups: blkio
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
W0219 17:10:33.672528   67048 out.go:239] 
W0219 17:10:33.672720   67048 out.go:239] 💣  Error starting cluster: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.24.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0219 09:06:32.018342    2456 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING SystemVerification]: missing optional cgroups: blkio
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

W0219 17:10:33.674017   67048 out.go:239] 
W0219 17:10:33.674675   67048 out.go:239] [31m╭───────────────────────────────────────────────────────────────────────────────────────────╮[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    😿  If the above advice does not help, please let us know:                             [31m│[0m
[31m│[0m    👉  https://github.com/kubernetes/minikube/issues/new/choose                           [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m│[0m    Please run `minikube logs --file=logs.txt` and attach logs.txt to the GitHub issue.    [31m│[0m
[31m│[0m                                                                                           [31m│[0m
[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯[0m
I0219 17:10:33.729497   67048 out.go:177] 
W0219 17:10:33.785468   67048 out.go:239] ❌  Exiting due to K8S_KUBELET_NOT_RUNNING: wait: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables": Process exited with status 1
stdout:
[init] Using Kubernetes version: v1.24.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/var/lib/minikube/certs"
[certs] Using existing ca certificate authority
[certs] Using existing apiserver certificate and key on disk
[certs] Using existing apiserver-kubelet-client certificate and key on disk
[certs] Using existing front-proxy-ca certificate authority
[certs] Using existing front-proxy-client certificate and key on disk
[certs] Using existing etcd/ca certificate authority
[certs] Using existing etcd/server certificate and key on disk
[certs] Using existing etcd/peer certificate and key on disk
[certs] Using existing etcd/healthcheck-client certificate and key on disk
[certs] Using existing apiserver-etcd-client certificate and key on disk
[certs] Using the existing "sa" key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'

stderr:
W0219 09:06:32.018342    2456 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/var/run/cri-dockerd.sock". Please update your configuration!
	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
	[WARNING SystemVerification]: missing optional cgroups: blkio
	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher

W0219 17:10:33.786871   67048 out.go:239] 💡  Suggestion: Check output of 'journalctl -xeu kubelet', try passing --extra-config=kubelet.cgroup-driver=systemd to minikube start
W0219 17:10:33.787130   67048 out.go:239] 🍿  Related issue: https://github.com/kubernetes/minikube/issues/4172
I0219 17:10:33.822448   67048 out.go:177] 

* 
* ==> Docker <==
* -- Logs begin at Sun 2023-02-19 07:11:33 UTC, end at Sun 2023-02-19 15:35:16 UTC. --
Feb 19 14:18:21 minikube dockerd[5411]: time="2023-02-19T14:18:21.552001418Z" level=info msg="ignoring event" container=a8bd5793b6d2448e4739c42c462a2ccfd2023811043b2678ce9aafd8d7d3935c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:19:48 minikube dockerd[5411]: time="2023-02-19T14:19:48.144103847Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": dial tcp 142.251.8.82:443: i/o timeout"
Feb 19 14:19:48 minikube dockerd[5411]: time="2023-02-19T14:19:48.144175430Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": dial tcp 142.251.8.82:443: i/o timeout"
Feb 19 14:19:48 minikube dockerd[5411]: time="2023-02-19T14:19:48.147456722Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": dial tcp 142.251.8.82:443: i/o timeout"
Feb 19 14:23:31 minikube dockerd[5411]: time="2023-02-19T14:23:31.592084714Z" level=info msg="ignoring event" container=662e057aeb938c815cc65edd6428cfb61713cacb5868ae746eb42a0516ae80ee module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:25:18 minikube dockerd[5411]: time="2023-02-19T14:25:18.141790305Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:25:18 minikube dockerd[5411]: time="2023-02-19T14:25:18.141823805Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:25:18 minikube dockerd[5411]: time="2023-02-19T14:25:18.142927972Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:28:34 minikube dockerd[5411]: time="2023-02-19T14:28:34.566109174Z" level=info msg="ignoring event" container=96a553c44d5071e7984d24ef79ae18bde405bb4db18139c57bf68dd087392e39 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:30:39 minikube dockerd[5411]: time="2023-02-19T14:30:39.160124051Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:30:39 minikube dockerd[5411]: time="2023-02-19T14:30:39.160211468Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:30:39 minikube dockerd[5411]: time="2023-02-19T14:30:39.162691884Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:33:46 minikube dockerd[5411]: time="2023-02-19T14:33:46.556874596Z" level=info msg="ignoring event" container=0118a5088a7c07dfe8ff47f6df490e97df8ad22ba20ab4a1ffb227a586871afe module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:36:01 minikube dockerd[5411]: time="2023-02-19T14:36:01.150906255Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": dial tcp 64.233.189.82:443: i/o timeout"
Feb 19 14:36:01 minikube dockerd[5411]: time="2023-02-19T14:36:01.150940714Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": dial tcp 64.233.189.82:443: i/o timeout"
Feb 19 14:36:01 minikube dockerd[5411]: time="2023-02-19T14:36:01.153316172Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": dial tcp 64.233.189.82:443: i/o timeout"
Feb 19 14:38:55 minikube dockerd[5411]: time="2023-02-19T14:38:55.555271544Z" level=info msg="ignoring event" container=b8ff73f9c6329e665c6123cffe048f00bbd201a866afa07bb5d46d5b1f9c0751 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:41:17 minikube dockerd[5411]: time="2023-02-19T14:41:17.160729555Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": context deadline exceeded"
Feb 19 14:41:17 minikube dockerd[5411]: time="2023-02-19T14:41:17.160793971Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": context deadline exceeded"
Feb 19 14:41:17 minikube dockerd[5411]: time="2023-02-19T14:41:17.163342430Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": context deadline exceeded"
Feb 19 14:44:02 minikube dockerd[5411]: time="2023-02-19T14:44:02.663053798Z" level=info msg="ignoring event" container=1dabe0cbb9b866e20ee2dac92b1ef21ab1bef4923038197a383346242af10662 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:46:37 minikube dockerd[5411]: time="2023-02-19T14:46:37.151689592Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:46:37 minikube dockerd[5411]: time="2023-02-19T14:46:37.151735050Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:46:37 minikube dockerd[5411]: time="2023-02-19T14:46:37.153441425Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:49:14 minikube dockerd[5411]: time="2023-02-19T14:49:14.541690845Z" level=info msg="ignoring event" container=9dad0713efd6c4583db7a003759e78f3421471c55b66568922fbe239b86ea001 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:52:05 minikube dockerd[5411]: time="2023-02-19T14:52:05.159618257Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:52:05 minikube dockerd[5411]: time="2023-02-19T14:52:05.159689132Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:52:05 minikube dockerd[5411]: time="2023-02-19T14:52:05.162674507Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:54:20 minikube dockerd[5411]: time="2023-02-19T14:54:20.533634709Z" level=info msg="ignoring event" container=b48cb1cc37b58f179c460f3841e8756ba43e1f1be1e9171d890b8155b1f102c2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 14:57:24 minikube dockerd[5411]: time="2023-02-19T14:57:24.151435085Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:57:24 minikube dockerd[5411]: time="2023-02-19T14:57:24.151488710Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:57:24 minikube dockerd[5411]: time="2023-02-19T14:57:24.153182585Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 14:59:24 minikube dockerd[5411]: time="2023-02-19T14:59:24.528396377Z" level=info msg="ignoring event" container=dd8283c0653a0f5f7643a29f68e2326197833c45e20930f9dc5a70e4be30e91f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:02:47 minikube dockerd[5411]: time="2023-02-19T15:02:47.166662055Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:02:47 minikube dockerd[5411]: time="2023-02-19T15:02:47.166720596Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:02:47 minikube dockerd[5411]: time="2023-02-19T15:02:47.170273305Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:04:28 minikube dockerd[5411]: time="2023-02-19T15:04:28.650608837Z" level=info msg="ignoring event" container=d1a4d3216ad64c5169daa58f0d1e3f0840dfea95e3e6fdea1cebb03767659ec7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:08:14 minikube dockerd[5411]: time="2023-02-19T15:08:14.151174970Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:08:14 minikube dockerd[5411]: time="2023-02-19T15:08:14.151232928Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:08:14 minikube dockerd[5411]: time="2023-02-19T15:08:14.154095970Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:09:34 minikube dockerd[5411]: time="2023-02-19T15:09:34.634152049Z" level=info msg="ignoring event" container=2ab1dce7658f2c78b8813f6bcb5431fc4f748607eebf3e614464e93ad02234ab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:13:42 minikube dockerd[5411]: time="2023-02-19T15:13:42.143731011Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:13:42 minikube dockerd[5411]: time="2023-02-19T15:13:42.143829302Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:13:42 minikube dockerd[5411]: time="2023-02-19T15:13:42.146619136Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:14:40 minikube dockerd[5411]: time="2023-02-19T15:14:40.647759801Z" level=info msg="ignoring event" container=e2716c6304cb68453a476a06c0a67642a8a45dcdc77602bb998524e7f2757a49 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:19:00 minikube dockerd[5411]: time="2023-02-19T15:19:00.139886505Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:19:00 minikube dockerd[5411]: time="2023-02-19T15:19:00.139930630Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:19:00 minikube dockerd[5411]: time="2023-02-19T15:19:00.162535880Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:19:45 minikube dockerd[5411]: time="2023-02-19T15:19:45.538673720Z" level=info msg="ignoring event" container=ec00bed864f2eec6eda5eb462e48e9ef2e3b186b67d853ea5d544caa81f08ab7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:24:24 minikube dockerd[5411]: time="2023-02-19T15:24:24.146784710Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:24:24 minikube dockerd[5411]: time="2023-02-19T15:24:24.146903752Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:24:24 minikube dockerd[5411]: time="2023-02-19T15:24:24.151038335Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:25:02 minikube dockerd[5411]: time="2023-02-19T15:25:02.542722881Z" level=info msg="ignoring event" container=05b1145bbbf22b6eadfa9207e50ad1f696ab766f2b8aff73a6e5597e4d79c5d5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:29:41 minikube dockerd[5411]: time="2023-02-19T15:29:41.138801968Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:29:41 minikube dockerd[5411]: time="2023-02-19T15:29:41.138905968Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:29:41 minikube dockerd[5411]: time="2023-02-19T15:29:41.140351427Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:30:10 minikube dockerd[5411]: time="2023-02-19T15:30:10.516813010Z" level=info msg="ignoring event" container=4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 15:35:10 minikube dockerd[5411]: time="2023-02-19T15:35:10.123903593Z" level=warning msg="Error getting v2 registry: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:35:10 minikube dockerd[5411]: time="2023-02-19T15:35:10.124009010Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"
Feb 19 15:35:10 minikube dockerd[5411]: time="2023-02-19T15:35:10.127307635Z" level=error msg="Handler for POST /v1.41/images/create returned error: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID
4d3b4e231e76a       a4fefcde6b458                                                                                          5 minutes ago       Exited              ambassador-operator         125                 3321afd18cc2a
74e1e8e8f1f28       registry@sha256:d5459fcb27aecc752520df4b492b08358a1912fcdfa454f7d2101d4b09991daa                       7 hours ago         Running             registry                    0                   bb5a3b5c1b971
0f9d2ae96631d       09df423d5a37f                                                                                          8 hours ago         Running             kubernetes-dashboard        7                   2dfb7ca09b9ca
521ed5f408ec3       d36a89daa1945                                                                                          8 hours ago         Running             kube-proxy                  5                   20801d41a62a1
7b4285f05bf77       edaa71f2aee88                                                                                          8 hours ago         Running             coredns                     5                   b1971cd7de183
f5ab8b1f23205       ba04bb24b9575                                                                                          8 hours ago         Running             storage-provisioner         7                   a53771f08560f
4b0d92ad3f1e6       4bad79a8953b4                                                                                          8 hours ago         Running             kube-scheduler              12                  e6b690a76fb26
c16b801e9236a       3e63a2140741e                                                                                          8 hours ago         Running             kube-controller-manager     12                  275799fea0ce8
90d3169955b24       8e7422f73cf36                                                                                          8 hours ago         Running             kube-apiserver              12                  12c3cea89ab1c
5bb9779652f44       1040f7790951c                                                                                          8 hours ago         Running             etcd                        12                  84403d0bddeb6
79be99156b2c7       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     8 hours ago         Running             auth                        4                   ad1b31127aab4
2b6894b084335       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     8 hours ago         Running             auth                        4                   b0a7666c0c30b
728c89cf69fbf       1040f7790951c                                                                                          8 hours ago         Exited              etcd                        11                  1bbe287282113
e3deddccbd6f2       a422e0e982356                                                                                          8 hours ago         Running             dashboard-metrics-scraper   4                   1eb7942929782
007c388253070       09df423d5a37f                                                                                          8 hours ago         Exited              kubernetes-dashboard        6                   2dfb7ca09b9ca
6afcf831171f4       edaa71f2aee88                                                                                          8 hours ago         Exited              coredns                     4                   5c1934c1d3bb2
39fb63082d680       3e63a2140741e                                                                                          8 hours ago         Exited              kube-controller-manager     11                  f7ef0895ed89c
d75f1c94fceec       d36a89daa1945                                                                                          8 hours ago         Exited              kube-proxy                  4                   9b42b4165e709
7bc5abcbaefe4       ba04bb24b9575                                                                                          8 hours ago         Created             storage-provisioner         6                   547855e93acc2
5f2c39076c9dc       8e7422f73cf36                                                                                          8 hours ago         Exited              kube-apiserver              11                  7b33377dee7cc
853fcf9aeb0f4       4bad79a8953b4                                                                                          8 hours ago         Exited              kube-scheduler              11                  84a8f0ad14fe1
4b9b459f122a2       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     8 hours ago         Exited              auth                        3                   2a296030141cf
d5de2b25b5ec0       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   8 hours ago         Exited              dashboard-metrics-scraper   3                   76541e04c1c5f
07d28f0a1c7da       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     8 hours ago         Exited              auth                        3                   26c71666218db

* 
* ==> coredns [6afcf831171f] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/arm64, go1.17.1, 13a9191
[INFO] plugin/health: Going into lameduck mode for 5s
[ERROR] plugin/errors: 2 283744096059473130.3646197341201287563. HINFO: dial udp 192.168.65.2:53: connect: network is unreachable
[ERROR] plugin/errors: 2 283744096059473130.3646197341201287563. HINFO: dial udp 192.168.65.2:53: connect: network is unreachable

* 
* ==> coredns [7b4285f05bf7] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/arm64, go1.17.1, 13a9191

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              <none>
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/primarv=true
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 12 Feb 2023 10:29:26 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 19 Feb 2023 15:35:11 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 19 Feb 2023 15:34:09 +0000   Sun, 12 Feb 2023 10:29:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 19 Feb 2023 15:34:09 +0000   Sun, 12 Feb 2023 10:29:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 19 Feb 2023 15:34:09 +0000   Sun, 12 Feb 2023 10:29:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 19 Feb 2023 15:34:09 +0000   Sun, 12 Feb 2023 10:29:36 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                5
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2036288Ki
  pods:               110
Allocatable:
  cpu:                5
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2036288Ki
  pods:               110
System Info:
  Machine ID:                 7f42765e713c4b909dde4d5f15b8d18f
  System UUID:                7f42765e713c4b909dde4d5f15b8d18f
  Boot ID:                    291575da-373f-4327-9afd-35a2ed4851f4
  Kernel Version:             5.10.76-linuxkit
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.3
  Kube-Proxy Version:         v1.23.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (15 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ambassador                  ambassador-operator-5dc77dccd7-wm7mc         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12h
  default                     auth-7c5dbfc47d-pdm75                        500m (10%!)(MISSING)    500m (10%!)(MISSING)  128Mi (6%!)(MISSING)       128Mi (6%!)(MISSING)     7d5h
  default                     auth-7ffb89668c-fbntt                        500m (10%!)(MISSING)    500m (10%!)(MISSING)  1Gi (51%!)(MISSING)        1Gi (51%!)(MISSING)      7d5h
  default                     rabbitmq-0                                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         22m
  kube-system                 coredns-64897985d-vl5tw                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (8%!)(MISSING)     7d5h
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (5%!)(MISSING)       0 (0%!)(MISSING)         7d5h
  kube-system                 kube-apiserver-minikube                      250m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
  kube-system                 kube-controller-manager-minikube             200m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
  kube-system                 kube-proxy-g9p5l                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
  kube-system                 registry-2rrq4                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h5m
  kube-system                 registry-proxy-ndxg8                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h5m
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
  kubernetes-dashboard        dashboard-metrics-scraper-65b4bd797-wv5c4    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
  kubernetes-dashboard        kubernetes-dashboard-cd7c84bfc-dgw9r         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7d5h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1750m (35%!)(MISSING)   1 (20%!)(MISSING)
  memory             1322Mi (66%!)(MISSING)  1322Mi (66%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Feb 7 00:14] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.027987] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[Feb 7 00:15] grpcfuse: loading out-of-tree module taints kernel.
[Feb 7 00:24] hrtimer: interrupt took 4337541 ns

* 
* ==> etcd [5bb9779652f4] <==
* {"level":"info","ts":"2023-02-19T13:08:35.606Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":542467}
{"level":"info","ts":"2023-02-19T13:08:35.607Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":542467,"took":"532.75µs"}
{"level":"info","ts":"2023-02-19T13:13:35.610Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":542698}
{"level":"info","ts":"2023-02-19T13:13:35.611Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":542698,"took":"859.792µs"}
{"level":"info","ts":"2023-02-19T13:18:35.616Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":542926}
{"level":"info","ts":"2023-02-19T13:18:35.617Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":542926,"took":"434.958µs"}
{"level":"info","ts":"2023-02-19T13:23:35.623Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":543153}
{"level":"info","ts":"2023-02-19T13:23:35.624Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":543153,"took":"716.458µs"}
{"level":"info","ts":"2023-02-19T13:28:35.627Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":543380}
{"level":"info","ts":"2023-02-19T13:28:35.628Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":543380,"took":"835.041µs"}
{"level":"info","ts":"2023-02-19T13:33:35.635Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":543609}
{"level":"info","ts":"2023-02-19T13:33:35.637Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":543609,"took":"1.324334ms"}
{"level":"info","ts":"2023-02-19T13:38:35.640Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":543836}
{"level":"info","ts":"2023-02-19T13:38:35.641Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":543836,"took":"716.542µs"}
{"level":"info","ts":"2023-02-19T13:43:35.646Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":544067}
{"level":"info","ts":"2023-02-19T13:43:35.646Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":544067,"took":"657.5µs"}
{"level":"info","ts":"2023-02-19T13:48:35.650Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":544294}
{"level":"info","ts":"2023-02-19T13:48:35.651Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":544294,"took":"969.209µs"}
{"level":"info","ts":"2023-02-19T13:53:35.654Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":544521}
{"level":"info","ts":"2023-02-19T13:53:35.654Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":544521,"took":"599.666µs"}
{"level":"info","ts":"2023-02-19T13:58:35.660Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":544749}
{"level":"info","ts":"2023-02-19T13:58:35.661Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":544749,"took":"782.667µs"}
{"level":"info","ts":"2023-02-19T14:03:35.665Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":544973}
{"level":"info","ts":"2023-02-19T14:03:35.666Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":544973,"took":"533.958µs"}
{"level":"info","ts":"2023-02-19T14:08:35.670Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":545200}
{"level":"info","ts":"2023-02-19T14:08:35.671Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":545200,"took":"735.084µs"}
{"level":"info","ts":"2023-02-19T14:13:35.674Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":545432}
{"level":"info","ts":"2023-02-19T14:13:35.675Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":545432,"took":"341.583µs"}
{"level":"info","ts":"2023-02-19T14:18:35.679Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":545659}
{"level":"info","ts":"2023-02-19T14:18:35.680Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":545659,"took":"672.833µs"}
{"level":"info","ts":"2023-02-19T14:23:35.684Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":545886}
{"level":"info","ts":"2023-02-19T14:23:35.685Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":545886,"took":"674.166µs"}
{"level":"info","ts":"2023-02-19T14:28:35.688Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":546113}
{"level":"info","ts":"2023-02-19T14:28:35.689Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":546113,"took":"393.542µs"}
{"level":"info","ts":"2023-02-19T14:33:35.692Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":546340}
{"level":"info","ts":"2023-02-19T14:33:35.693Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":546340,"took":"508.75µs"}
{"level":"info","ts":"2023-02-19T14:38:35.702Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":546556}
{"level":"info","ts":"2023-02-19T14:38:35.702Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":546556,"took":"447.666µs"}
{"level":"info","ts":"2023-02-19T14:43:35.706Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":546788}
{"level":"info","ts":"2023-02-19T14:43:35.707Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":546788,"took":"696µs"}
{"level":"info","ts":"2023-02-19T14:48:35.711Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":547015}
{"level":"info","ts":"2023-02-19T14:48:35.712Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":547015,"took":"696.125µs"}
{"level":"info","ts":"2023-02-19T14:53:35.718Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":547243}
{"level":"info","ts":"2023-02-19T14:53:35.718Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":547243,"took":"446.125µs"}
{"level":"info","ts":"2023-02-19T14:58:35.723Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":547470}
{"level":"info","ts":"2023-02-19T14:58:35.723Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":547470,"took":"315.625µs"}
{"level":"info","ts":"2023-02-19T15:03:35.731Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":547697}
{"level":"info","ts":"2023-02-19T15:03:35.734Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":547697,"took":"2.11725ms"}
{"level":"info","ts":"2023-02-19T15:08:35.735Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":547925}
{"level":"info","ts":"2023-02-19T15:08:35.736Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":547925,"took":"874.208µs"}
{"level":"info","ts":"2023-02-19T15:13:35.715Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":548155}
{"level":"info","ts":"2023-02-19T15:13:35.717Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":548155,"took":"1.289459ms"}
{"level":"info","ts":"2023-02-19T15:18:35.722Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":548417}
{"level":"info","ts":"2023-02-19T15:18:35.723Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":548417,"took":"1.443292ms"}
{"level":"info","ts":"2023-02-19T15:23:35.725Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":548663}
{"level":"info","ts":"2023-02-19T15:23:35.726Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":548663,"took":"671.042µs"}
{"level":"info","ts":"2023-02-19T15:28:35.729Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":548890}
{"level":"info","ts":"2023-02-19T15:28:35.730Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":548890,"took":"870.417µs"}
{"level":"info","ts":"2023-02-19T15:33:35.733Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":549119}
{"level":"info","ts":"2023-02-19T15:33:35.734Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":549119,"took":"1.126833ms"}

* 
* ==> etcd [728c89cf69fb] <==
* {"level":"warn","ts":1676790989.5025098,"caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-02-19T07:16:29.502Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-02-19T07:16:29.502Z","caller":"etcdmain/etcd.go:115","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-02-19T07:16:29.502Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-02-19T07:16:29.503Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-02-19T07:16:29.503Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-02-19T07:16:29.503Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.1","git-sha":"d42e8589e","go-version":"go1.16.2","go-os":"linux","go-arch":"arm64","max-cpu-set":5,"max-cpu-available":5,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-02-19T07:16:29.565Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"61.683416ms"}
{"level":"info","ts":"2023-02-19T07:16:30.642Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":680068,"snapshot-size":"10 kB"}
{"level":"info","ts":"2023-02-19T07:16:30.642Z","caller":"etcdserver/server.go:518","msg":"recovered v3 backend from snapshot","backend-size-bytes":6598656,"backend-size":"6.6 MB","backend-size-in-use-bytes":4620288,"backend-size-in-use":"4.6 MB"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":681932}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 12"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 12, commit: 681932, applied: 680068, lastindex: 681932, lastterm: 12]"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-02-19T07:16:30.728Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-02-19T07:16:30.729Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":527642}
{"level":"info","ts":"2023-02-19T07:16:30.730Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":528430}
{"level":"info","ts":"2023-02-19T07:16:30.731Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-02-19T07:16:30.731Z","caller":"etcdserver/server.go:834","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.1","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-02-19T07:16:30.732Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 12"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 12"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 12"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-02-19T07:16:31.634Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-02-19T07:16:31.635Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-02-19T07:16:33.532Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-02-19T07:16:33.532Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2023/02/19 07:16:33 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2023/02/19 07:16:33 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2023-02-19T07:16:33.535Z","caller":"etcdserver/server.go:1438","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-02-19T07:16:33.537Z","caller":"embed/etcd.go:562","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:33.538Z","caller":"embed/etcd.go:567","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:33.538Z","caller":"embed/etcd.go:369","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  15:35:18 up 12 days, 15:20,  0 users,  load average: 0.14, 0.20, 0.23
Linux minikube 5.10.76-linuxkit #1 SMP PREEMPT Mon Nov 8 11:22:26 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [5f2c39076c9d] <==
* I0219 07:16:07.704602       1 server.go:565] external host was not specified, using 192.168.49.2
I0219 07:16:07.705080       1 server.go:172] Version: v1.23.3
I0219 07:16:08.003635       1 shared_informer.go:240] Waiting for caches to sync for node_authorizer
I0219 07:16:08.004993       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0219 07:16:08.005008       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
I0219 07:16:08.005481       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0219 07:16:08.005495       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0219 07:16:08.008029       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:09.004666       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:09.008306       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:10.005617       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:10.650134       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:11.705844       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:13.175314       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:14.309459       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:17.171073       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:18.582999       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:23.775138       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:24.133153       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-apiserver [90d3169955b2] <==
* I0219 07:16:46.142460       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0219 07:16:46.158362       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0219 07:16:46.745925       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0219 07:16:46.745961       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0219 07:16:46.746290       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0219 07:16:46.746349       1 secure_serving.go:266] Serving securely on [::]:8443
I0219 07:16:46.746381       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0219 07:16:46.746874       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0219 07:16:46.747833       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0219 07:16:46.750543       1 available_controller.go:491] Starting AvailableConditionController
I0219 07:16:46.750557       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0219 07:16:46.750566       1 controller.go:83] Starting OpenAPI AggregationController
I0219 07:16:46.750583       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I0219 07:16:46.751596       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0219 07:16:46.751609       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0219 07:16:46.767464       1 autoregister_controller.go:141] Starting autoregister controller
I0219 07:16:46.767484       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0219 07:16:46.767507       1 controller.go:85] Starting OpenAPI controller
I0219 07:16:46.767516       1 naming_controller.go:291] Starting NamingConditionController
I0219 07:16:46.767525       1 establishing_controller.go:76] Starting EstablishingController
I0219 07:16:46.767536       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0219 07:16:46.767549       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0219 07:16:46.767999       1 crd_finalizer.go:266] Starting CRDFinalizer
I0219 07:16:46.768224       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0219 07:16:46.768473       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0219 07:16:46.768681       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0219 07:16:46.768723       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0219 07:16:46.768750       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0219 07:16:46.768796       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
E0219 07:16:46.779756       1 controller.go:157] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0219 07:16:46.786460       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0219 07:16:46.788632       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0219 07:16:46.859144       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0219 07:16:46.859430       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0219 07:16:46.859678       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0219 07:16:46.867595       1 cache.go:39] Caches are synced for autoregister controller
I0219 07:16:46.869635       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I0219 07:16:46.869650       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0219 07:16:46.980022       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0219 07:16:47.746450       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0219 07:16:47.746488       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0219 07:16:47.771718       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I0219 07:16:48.413785       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0219 07:16:48.418007       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0219 07:16:48.434522       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0219 07:16:48.441680       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0219 07:16:48.444598       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0219 07:16:59.917450       1 controller.go:611] quota admission added evaluator for: endpoints
I0219 07:16:59.965039       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0219 08:30:04.959541       1 alloc.go:329] "allocated clusterIPs" service="kube-system/registry" clusterIPs=map[IPv4:10.101.79.164]
I0219 08:30:05.036326       1 controller.go:611] quota admission added evaluator for: controllerrevisions.apps
E0219 11:30:27.484294       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0219 11:30:27.484313       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0219 11:46:22.490282       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0219 11:46:22.490282       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0219 11:46:22.491455       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0219 11:46:22.530394       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
W0219 15:12:29.623112       1 dispatcher.go:153] Failed calling webhook, failing closed validate.nginx.ingress.kubernetes.io: failed calling webhook "validate.nginx.ingress.kubernetes.io": failed to call webhook: Post "https://ingress-nginx-controller-admission.ingress-nginx.svc:443/networking/v1/ingresses?timeout=10s": dial tcp 10.101.80.218:443: connect: connection refused
I0219 15:12:29.653287       1 alloc.go:329] "allocated clusterIPs" service="default/rabbitmq" clusterIPs=map[IPv4:10.103.125.3]
I0219 15:12:29.666939       1 controller.go:611] quota admission added evaluator for: statefulsets.apps

* 
* ==> kube-controller-manager [39fb63082d68] <==
* I0219 07:16:30.002488       1 serving.go:348] Generated self-signed cert in-memory
I0219 07:16:30.418990       1 controllermanager.go:196] Version: v1.23.3
I0219 07:16:30.421432       1 secure_serving.go:200] Serving securely on 127.0.0.1:10257
I0219 07:16:30.421411       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0219 07:16:30.421416       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0219 07:16:30.421575       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"

* 
* ==> kube-controller-manager [c16b801e9236] <==
* I0219 07:16:59.782763       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0219 07:16:59.786461       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I0219 07:16:59.791552       1 shared_informer.go:247] Caches are synced for ephemeral 
I0219 07:16:59.793767       1 shared_informer.go:247] Caches are synced for endpoint 
I0219 07:16:59.795966       1 shared_informer.go:247] Caches are synced for GC 
I0219 07:16:59.797048       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0219 07:16:59.802264       1 shared_informer.go:247] Caches are synced for HPA 
I0219 07:16:59.805432       1 shared_informer.go:247] Caches are synced for persistent volume 
I0219 07:16:59.806757       1 shared_informer.go:247] Caches are synced for PV protection 
I0219 07:16:59.813192       1 shared_informer.go:247] Caches are synced for PVC protection 
I0219 07:16:59.813208       1 shared_informer.go:247] Caches are synced for job 
I0219 07:16:59.813459       1 shared_informer.go:247] Caches are synced for service account 
I0219 07:16:59.814528       1 shared_informer.go:247] Caches are synced for taint 
I0219 07:16:59.814574       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
W0219 07:16:59.814603       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0219 07:16:59.814605       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0219 07:16:59.814617       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I0219 07:16:59.814648       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0219 07:16:59.815141       1 shared_informer.go:247] Caches are synced for TTL 
I0219 07:16:59.815186       1 shared_informer.go:247] Caches are synced for namespace 
I0219 07:16:59.819196       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I0219 07:16:59.821897       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0219 07:16:59.856318       1 shared_informer.go:247] Caches are synced for node 
I0219 07:16:59.856344       1 range_allocator.go:173] Starting range CIDR allocator
I0219 07:16:59.856346       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I0219 07:16:59.856350       1 shared_informer.go:247] Caches are synced for cidrallocator 
I0219 07:16:59.861126       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0219 07:16:59.863498       1 shared_informer.go:247] Caches are synced for stateful set 
I0219 07:16:59.863551       1 shared_informer.go:247] Caches are synced for deployment 
I0219 07:16:59.863611       1 shared_informer.go:247] Caches are synced for daemon sets 
I0219 07:16:59.863731       1 shared_informer.go:247] Caches are synced for expand 
I0219 07:16:59.865819       1 shared_informer.go:247] Caches are synced for TTL after finished 
I0219 07:16:59.865909       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0219 07:16:59.871821       1 shared_informer.go:247] Caches are synced for disruption 
I0219 07:16:59.871840       1 disruption.go:371] Sending events to api server.
I0219 07:16:59.881603       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I0219 07:16:59.881670       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I0219 07:16:59.881695       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I0219 07:16:59.883765       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I0219 07:16:59.912925       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I0219 07:16:59.969765       1 shared_informer.go:247] Caches are synced for resource quota 
I0219 07:16:59.970610       1 shared_informer.go:247] Caches are synced for resource quota 
I0219 07:16:59.999716       1 shared_informer.go:247] Caches are synced for cronjob 
I0219 07:17:00.063650       1 shared_informer.go:247] Caches are synced for crt configmap 
I0219 07:17:00.063702       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I0219 07:17:00.090810       1 shared_informer.go:247] Caches are synced for attach detach 
I0219 07:17:00.478250       1 shared_informer.go:247] Caches are synced for garbage collector 
I0219 07:17:00.478301       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0219 07:17:00.487282       1 shared_informer.go:247] Caches are synced for garbage collector 
I0219 08:30:04.973270       1 event.go:294] "Event occurred" object="kube-system/registry" kind="ReplicationController" apiVersion="v1" type="Normal" reason="SuccessfulCreate" message="Created pod: registry-2rrq4"
I0219 08:30:05.041490       1 event.go:294] "Event occurred" object="kube-system/registry-proxy" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: registry-proxy-ndxg8"
I0219 09:57:43.835834       1 event.go:294] "Event occurred" object="ingress-nginx/ingress-nginx-controller-56f5896d55" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-56f5896d55-zcxj7"
E0219 11:30:27.488698       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0219 11:30:27.488990       1 garbagecollector.go:709] failed to discover preferred resources: Unauthorized
I0219 11:46:22.492610       1 event.go:294] "Event occurred" object="ambassador/ambassador-operator-metrics" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint ambassador/ambassador-operator-metrics: Unauthorized"
I0219 11:46:22.493714       1 event.go:294] "Event occurred" object="ambassador/ambassador-operator-metrics" kind="Service" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpointSlices" message="Error updating Endpoint Slices for Service ambassador/ambassador-operator-metrics: failed to update ambassador-operator-metrics-tjlhd EndpointSlice for Service ambassador/ambassador-operator-metrics: Unauthorized"
W0219 11:46:22.494547       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "ambassador/ambassador-operator-metrics", retrying. Error: failed to update ambassador-operator-metrics-tjlhd EndpointSlice for Service ambassador/ambassador-operator-metrics: Unauthorized
I0219 15:12:29.652863       1 event.go:294] "Event occurred" object="default/rabbitmq-pvc" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0219 15:12:29.653755       1 event.go:294] "Event occurred" object="default/rabbitmq-pvc" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="waiting for a volume to be created, either by external provisioner \"k8s.io/minikube-hostpath\" or manually created by system administrator"
I0219 15:12:29.746979       1 event.go:294] "Event occurred" object="default/rabbitmq" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod rabbitmq-0 in StatefulSet rabbitmq successful"

* 
* ==> kube-proxy [521ed5f408ec] <==
* I0219 07:16:51.024400       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0219 07:16:51.024450       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0219 07:16:51.024471       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0219 07:16:51.087048       1 server_others.go:206] "Using iptables Proxier"
I0219 07:16:51.087110       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0219 07:16:51.087119       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0219 07:16:51.087160       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0219 07:16:51.088375       1 server.go:656] "Version info" version="v1.23.3"
I0219 07:16:51.090166       1 config.go:317] "Starting service config controller"
I0219 07:16:51.090461       1 config.go:226] "Starting endpoint slice config controller"
I0219 07:16:51.090667       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0219 07:16:51.090875       1 shared_informer.go:240] Waiting for caches to sync for service config
I0219 07:16:51.191116       1 shared_informer.go:247] Caches are synced for service config 
I0219 07:16:51.191118       1 shared_informer.go:247] Caches are synced for endpoint slice config 
E0219 07:16:51.221356       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30244: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:http IP: IPFamily:4 Port:30244 Protocol:TCP}
E0219 07:16:51.221527       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30509: bind: address already in use" port={Description:nodePort for default/hello-minikube IP: IPFamily:4 Port:30509 Protocol:TCP}
E0219 07:16:51.221642       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32210: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:https IP: IPFamily:4 Port:32210 Protocol:TCP}

* 
* ==> kube-proxy [d75f1c94fcee] <==
* E0219 07:16:28.884935       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0219 07:16:29.957931       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0219 07:16:32.251612       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused

* 
* ==> kube-scheduler [4b0d92ad3f1e] <==
* I0219 07:16:45.000471       1 serving.go:348] Generated self-signed cert in-memory
W0219 07:16:46.795475       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0219 07:16:46.796529       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0219 07:16:46.796914       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0219 07:16:46.796925       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0219 07:16:46.860053       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.3"
I0219 07:16:46.873812       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0219 07:16:46.874528       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0219 07:16:46.874090       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0219 07:16:46.881790       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0219 07:16:46.982265       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [853fcf9aeb0f] <==
* I0219 07:16:07.824157       1 serving.go:348] Generated self-signed cert in-memory
W0219 07:16:18.008982       1 authentication.go:345] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": net/http: TLS handshake timeout
W0219 07:16:18.009203       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0219 07:16:18.009225       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false

* 
* ==> kubelet <==
* -- Logs begin at Sun 2023-02-19 07:11:33 UTC, end at Sun 2023-02-19 15:35:19 UTC. --
Feb 19 15:32:49 minikube kubelet[7635]: E0219 15:32:49.090903    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:32:56 minikube kubelet[7635]: E0219 15:32:56.090832    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:32:58 minikube kubelet[7635]: I0219 15:32:58.088061    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:32:58 minikube kubelet[7635]: E0219 15:32:58.088346    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:33:01 minikube kubelet[7635]: E0219 15:33:01.090015    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:33:01 minikube kubelet[7635]: E0219 15:33:01.090053    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:33:08 minikube kubelet[7635]: E0219 15:33:08.090175    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:33:09 minikube kubelet[7635]: I0219 15:33:09.087513    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:33:09 minikube kubelet[7635]: E0219 15:33:09.087753    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:33:16 minikube kubelet[7635]: E0219 15:33:16.088849    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:33:16 minikube kubelet[7635]: E0219 15:33:16.088882    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:33:19 minikube kubelet[7635]: E0219 15:33:19.089166    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:33:24 minikube kubelet[7635]: I0219 15:33:24.088721    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:33:24 minikube kubelet[7635]: E0219 15:33:24.088963    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:33:27 minikube kubelet[7635]: E0219 15:33:27.089170    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:33:27 minikube kubelet[7635]: E0219 15:33:27.089207    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:33:33 minikube kubelet[7635]: W0219 15:33:33.065657    7635 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 19 15:33:33 minikube kubelet[7635]: W0219 15:33:33.066238    7635 machine.go:65] Cannot read vendor id correctly, set empty.
Feb 19 15:33:33 minikube kubelet[7635]: E0219 15:33:33.089418    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:33:37 minikube kubelet[7635]: I0219 15:33:37.087732    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:33:37 minikube kubelet[7635]: E0219 15:33:37.087959    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:33:41 minikube kubelet[7635]: E0219 15:33:41.089295    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:33:41 minikube kubelet[7635]: E0219 15:33:41.089325    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:33:47 minikube kubelet[7635]: E0219 15:33:47.089210    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:33:48 minikube kubelet[7635]: I0219 15:33:48.088093    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:33:48 minikube kubelet[7635]: E0219 15:33:48.088281    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:33:52 minikube kubelet[7635]: E0219 15:33:52.089430    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:33:52 minikube kubelet[7635]: E0219 15:33:52.089464    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:34:00 minikube kubelet[7635]: E0219 15:34:00.088742    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:34:02 minikube kubelet[7635]: I0219 15:34:02.087291    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:34:02 minikube kubelet[7635]: E0219 15:34:02.087468    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:34:06 minikube kubelet[7635]: E0219 15:34:06.095125    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:34:06 minikube kubelet[7635]: E0219 15:34:06.095272    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:34:12 minikube kubelet[7635]: E0219 15:34:12.089598    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:34:16 minikube kubelet[7635]: I0219 15:34:16.087571    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:34:16 minikube kubelet[7635]: E0219 15:34:16.088247    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:34:21 minikube kubelet[7635]: E0219 15:34:21.092145    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:34:21 minikube kubelet[7635]: E0219 15:34:21.092214    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:34:26 minikube kubelet[7635]: E0219 15:34:26.089449    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:34:30 minikube kubelet[7635]: I0219 15:34:30.087322    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:34:30 minikube kubelet[7635]: E0219 15:34:30.087653    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:34:35 minikube kubelet[7635]: E0219 15:34:35.090236    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:34:35 minikube kubelet[7635]: E0219 15:34:35.090282    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:34:41 minikube kubelet[7635]: E0219 15:34:41.089299    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ImagePullBackOff: \"Back-off pulling image \\\"gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da\\\"\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:34:43 minikube kubelet[7635]: I0219 15:34:43.087575    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:34:43 minikube kubelet[7635]: E0219 15:34:43.087899    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:34:49 minikube kubelet[7635]: E0219 15:34:49.089455    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:34:49 minikube kubelet[7635]: E0219 15:34:49.089514    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:34:56 minikube kubelet[7635]: I0219 15:34:56.087780    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:34:56 minikube kubelet[7635]: E0219 15:34:56.087955    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:35:00 minikube kubelet[7635]: E0219 15:35:00.089796    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:35:00 minikube kubelet[7635]: E0219 15:35:00.089841    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf
Feb 19 15:35:09 minikube kubelet[7635]: I0219 15:35:09.087482    7635 scope.go:110] "RemoveContainer" containerID="4d3b4e231e76a3086d47e04f1f25168582aa3f3a1df1cf57df2dcd2703b2385b"
Feb 19 15:35:09 minikube kubelet[7635]: E0219 15:35:09.088130    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 15:35:10 minikube kubelet[7635]: E0219 15:35:10.128203    7635 remote_image.go:216] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da"
Feb 19 15:35:10 minikube kubelet[7635]: E0219 15:35:10.128258    7635 kuberuntime_image.go:51] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://gcr.io/v2/\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" image="gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da"
Feb 19 15:35:10 minikube kubelet[7635]: E0219 15:35:10.128377    7635 kuberuntime_manager.go:918] container &Container{Name:registry-proxy,Image:gcr.io/google_containers/kube-registry-proxy:0.4@sha256:1040f25a5273de0d72c54865a8efd47e3292de9fb8e5353e3fa76736b854f2da,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:registry,HostPort:5000,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:REGISTRY_HOST,Value:registry.kube-system.svc.cluster.local,ValueFrom:nil,},EnvVar{Name:REGISTRY_PORT,Value:80,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m5gv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod registry-proxy-ndxg8_kube-system(e12604cf-92ea-47f7-9f37-de011306c5ad): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: Get "https://gcr.io/v2/": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
Feb 19 15:35:10 minikube kubelet[7635]: E0219 15:35:10.128415    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"registry-proxy\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: Get \\\"https://gcr.io/v2/\\\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"" pod="kube-system/registry-proxy-ndxg8" podUID=e12604cf-92ea-47f7-9f37-de011306c5ad
Feb 19 15:35:12 minikube kubelet[7635]: E0219 15:35:12.088263    7635 kuberuntime_manager.go:918] container &Container{Name:rabbitmq,Image:rabbitmq:3-management,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:15672,Protocol:TCP,HostIP:,},ContainerPort{Name:amqp,HostPort:0,ContainerPort:5672,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:rabbitmq-volume,ReadOnly:false,MountPath:/var/lib/rabbitmq,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-lgx7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-configmap,},Optional:nil,},SecretRef:nil,},EnvFromSource{Prefix:,ConfigMapRef:nil,SecretRef:&SecretEnvSource{LocalObjectReference:LocalObjectReference{Name:rabbitmq-secret,},Optional:nil,},},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod rabbitmq-0_default(a5b7b1ec-e52f-4aac-91d4-b23deca601bf): CreateContainerConfigError: configmap "rabbitmq-configmap" not found
Feb 19 15:35:12 minikube kubelet[7635]: E0219 15:35:12.088301    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"rabbitmq\" with CreateContainerConfigError: \"configmap \\\"rabbitmq-configmap\\\" not found\"" pod="default/rabbitmq-0" podUID=a5b7b1ec-e52f-4aac-91d4-b23deca601bf

* 
* ==> kubernetes-dashboard [007c38825307] <==
* 2023/02/19 07:16:29 Using namespace: kubernetes-dashboard
2023/02/19 07:16:29 Using in-cluster config to connect to apiserver
2023/02/19 07:16:29 Using secret token for csrf signing
2023/02/19 07:16:29 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/02/19 07:16:29 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0x40004dfae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x358
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0x40001bd800)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x8c
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x40001bd800)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x40
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:95 +0x1dc

* 
* ==> kubernetes-dashboard [0f9d2ae96631] <==
* 2023/02/19 07:16:51 Using namespace: kubernetes-dashboard
2023/02/19 07:16:51 Using in-cluster config to connect to apiserver
2023/02/19 07:16:51 Using secret token for csrf signing
2023/02/19 07:16:51 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/02/19 07:16:51 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/02/19 07:16:51 Successful initial request to the apiserver, version: v1.23.3
2023/02/19 07:16:51 Generating JWE encryption key
2023/02/19 07:16:51 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/02/19 07:16:51 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/02/19 07:16:51 Initializing JWE encryption key from synchronized object
2023/02/19 07:16:51 Creating in-cluster Sidecar client
2023/02/19 07:16:51 Serving insecurely on HTTP port: 9090
2023/02/19 07:16:51 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/02/19 07:17:21 Successful request to sidecar
2023/02/19 07:16:51 Starting overwatch

* 
* ==> storage-provisioner [7bc5abcbaefe] <==
* 
* 
* ==> storage-provisioner [f5ab8b1f2320] <==
* I0219 07:16:50.207542       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0219 07:16:50.222446       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0219 07:16:50.223061       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0219 07:17:07.675677       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0219 07:17:07.675908       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"5d18e438-eb11-47ce-813c-bda0fe98c604", APIVersion:"v1", ResourceVersion:"528554", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b4ae6946-efef-432a-86df-32380dbe666b became leader
I0219 07:17:07.676133       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b4ae6946-efef-432a-86df-32380dbe666b!
I0219 07:17:07.779317       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b4ae6946-efef-432a-86df-32380dbe666b!
I0219 15:12:29.657505       1 controller.go:1332] provision "default/rabbitmq-pvc" class "standard": started
I0219 15:12:29.666562       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"rabbitmq-pvc", UID:"e3ee7776-8655-44bb-9152-fbbad1154f3a", APIVersion:"v1", ResourceVersion:"548339", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/rabbitmq-pvc"
I0219 15:12:29.664849       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    77ef37a3-b914-4e2c-ada2-46089c879084 342 0 2022-08-03 03:54:16 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2022-08-03 03:54:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-e3ee7776-8655-44bb-9152-fbbad1154f3a &PersistentVolumeClaim{ObjectMeta:{rabbitmq-pvc  default  e3ee7776-8655-44bb-9152-fbbad1154f3a 548339 0 2023-02-19 15:12:29 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"rabbitmq-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}},"storageClassName":"standard"}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2023-02-19 15:12:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2023-02-19 15:12:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:storageClassName":{},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{1073741824 0} {<nil>} 1Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/rabbitmq-pvc
I0219 15:12:29.678567       1 controller.go:1439] provision "default/rabbitmq-pvc" class "standard": volume "pvc-e3ee7776-8655-44bb-9152-fbbad1154f3a" provisioned
I0219 15:12:29.678591       1 controller.go:1456] provision "default/rabbitmq-pvc" class "standard": succeeded
I0219 15:12:29.678596       1 volume_store.go:212] Trying to save persistentvolume "pvc-e3ee7776-8655-44bb-9152-fbbad1154f3a"
I0219 15:12:29.687461       1 volume_store.go:219] persistentvolume "pvc-e3ee7776-8655-44bb-9152-fbbad1154f3a" saved
I0219 15:12:29.688004       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"rabbitmq-pvc", UID:"e3ee7776-8655-44bb-9152-fbbad1154f3a", APIVersion:"v1", ResourceVersion:"548339", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-e3ee7776-8655-44bb-9152-fbbad1154f3a

