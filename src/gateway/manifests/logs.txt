* 
* ==> Audit <==
* |----------------|----------------------|----------|---------|---------|-------------------------------|-------------------------------|
|    Command     |         Args         | Profile  |  User   | Version |          Start Time           |           End Time            |
|----------------|----------------------|----------|---------|---------|-------------------------------|-------------------------------|
| start          |                      | minikube | mikexie | v1.26.1 | 03 Aug 22 11:50 CST           | 03 Aug 22 11:54 CST           |
| start          |                      | minikube | mikexie | v1.26.1 | 03 Aug 22 11:54 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 03 Aug 22 11:56 CST           | 03 Aug 22 11:59 CST           |
| kubectl        | -- get po -A         | minikube | mikexie | v1.26.1 | 03 Aug 22 12:13 CST           |                               |
| stop           |                      | minikube | mikexie | v1.26.1 | 03 Aug 22 12:14 CST           |                               |
| kubectl        | -- get po -A         | minikube | mikexie | v1.26.1 | 03 Aug 22 12:15 CST           |                               |
| dashboard      |                      | minikube | mikexie | v1.26.1 | 03 Aug 22 12:16 CST           |                               |
| stop           |                      | minikube | mikexie | v1.26.1 | 04 Aug 22 08:18 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 04 Aug 22 08:19 CST           | 04 Aug 22 08:23 CST           |
| stop           |                      | minikube | mikexie | v1.26.1 | 04 Aug 22 08:26 CST           | 04 Aug 22 08:26 CST           |
| kubectl        | -- get po -A         | minikube | mikexie | v1.26.1 | 04 Aug 22 09:06 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 04 Aug 22 09:08 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 04 Aug 22 09:09 CST           | 04 Aug 22 09:13 CST           |
| dashboard      |                      | minikube | mikexie | v1.26.1 | 04 Aug 22 09:19 CST           |                               |
| service        | hello-minikube --url | minikube | mikexie | v1.26.1 | 04 Aug 22 09:20 CST           |                               |
| update-check   |                      | minikube | mikexie | v1.25.2 | Tue, 25 Oct 2022 10:04:58 CST | Tue, 25 Oct 2022 10:04:58 CST |
| update-check   |                      | minikube | mikexie | v1.25.2 | Tue, 25 Oct 2022 13:09:03 CST | Tue, 25 Oct 2022 13:09:03 CST |
| update-check   |                      | minikube | mikexie | v1.25.2 | Thu, 27 Oct 2022 00:30:56 CST | Thu, 27 Oct 2022 00:30:57 CST |
| update-check   |                      | minikube | mikexie | v1.25.2 | Thu, 17 Nov 2022 14:57:57 CST | Thu, 17 Nov 2022 14:57:58 CST |
| update-check   |                      | minikube | mikexie | v1.25.2 | Sat, 19 Nov 2022 16:59:41 CST | Sat, 19 Nov 2022 16:59:41 CST |
| update-check   |                      | minikube | mikexie | v1.25.2 | Tue, 20 Dec 2022 12:26:24 CST | Tue, 20 Dec 2022 12:26:24 CST |
| start          |                      | minikube | mikexie | v1.26.1 | 06 Jan 23 00:34 CST           |                               |
| kubectl        | -- get po -A         | minikube | mikexie | v1.26.1 | 06 Jan 23 00:54 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 06 Jan 23 00:54 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 06 Jan 23 01:09 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 06 Jan 23 01:11 CST           | 06 Jan 23 01:13 CST           |
| update-check   |                      | minikube | mikexie | v1.25.2 | Mon, 06 Feb 2023 23:57:04 CST | Mon, 06 Feb 2023 23:57:04 CST |
| update-check   |                      | minikube | mikexie | v1.25.2 | Sat, 11 Feb 2023 23:45:08 CST | Sat, 11 Feb 2023 23:45:10 CST |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 00:03 CST           | 12 Feb 23 00:08 CST           |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 13:35 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 14:38 CST           |                               |
| update-context |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 14:50 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 14:51 CST           |                               |
| stop           |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 15:21 CST           | 12 Feb 23 15:21 CST           |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 15:39 CST           |                               |
| start          | --driver=docker      | minikube | mikexie | v1.26.1 | 12 Feb 23 15:54 CST           |                               |
| start          | --driver=docker      | minikube | mikexie | v1.26.1 | 12 Feb 23 15:57 CST           | 12 Feb 23 16:01 CST           |
| stop           |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 18:26 CST           | 12 Feb 23 18:26 CST           |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 18:27 CST           | 12 Feb 23 18:29 CST           |
| stop           |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 19:28 CST           | 12 Feb 23 19:28 CST           |
| start          |                      | minikube | mikexie | v1.26.1 | 12 Feb 23 19:45 CST           | 12 Feb 23 19:48 CST           |
| addons         | list                 | minikube | mikexie | v1.26.1 | 19 Feb 23 00:29 CST           | 19 Feb 23 00:29 CST           |
| addons         | enbale ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 00:30 CST           | 19 Feb 23 00:30 CST           |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 00:30 CST           |                               |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 00:43 CST           |                               |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 01:21 CST           |                               |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 11:12 CST           |                               |
| addons         | list                 | minikube | mikexie | v1.26.1 | 19 Feb 23 11:21 CST           | 19 Feb 23 11:21 CST           |
| addons         | enable ambassador    | minikube | mikexie | v1.26.1 | 19 Feb 23 11:22 CST           |                               |
| addons         | list                 | minikube | mikexie | v1.26.1 | 19 Feb 23 11:24 CST           | 19 Feb 23 11:24 CST           |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 11:24 CST           |                               |
| addons         | list                 | minikube | mikexie | v1.26.1 | 19 Feb 23 11:58 CST           | 19 Feb 23 11:58 CST           |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 11:58 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 19 Feb 23 14:12 CST           | 19 Feb 23 14:15 CST           |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 14:16 CST           |                               |
| start          |                      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:08 CST           | 19 Feb 23 15:11 CST           |
| addons         | list                 | minikube | mikexie | v1.26.1 | 19 Feb 23 15:12 CST           | 19 Feb 23 15:12 CST           |
| start          |                      | minikube | mikexie | v1.26.1 | 19 Feb 23 15:12 CST           | 19 Feb 23 15:16 CST           |
| addons         | list                 | minikube | mikexie | v1.26.1 | 19 Feb 23 15:18 CST           | 19 Feb 23 15:18 CST           |
| addons         | enable ingress       | minikube | mikexie | v1.26.1 | 19 Feb 23 15:18 CST           |                               |
|----------------|----------------------|----------|---------|---------|-------------------------------|-------------------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/02/19 15:12:47
Running on machine: Mikes-MBP
Binary: Built with gc go1.18.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0219 15:12:47.421080   39506 out.go:296] Setting OutFile to fd 1 ...
I0219 15:12:47.421237   39506 out.go:348] isatty.IsTerminal(1) = true
I0219 15:12:47.421241   39506 out.go:309] Setting ErrFile to fd 2...
I0219 15:12:47.421245   39506 out.go:348] isatty.IsTerminal(2) = true
I0219 15:12:47.444206   39506 out.go:177] [31m‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ[0m
[31m‚îÇ[0m                                                                                                          [31m‚îÇ[0m
[31m‚îÇ[0m    You are trying to run the amd64 binary on an M1 system.                                               [31m‚îÇ[0m
[31m‚îÇ[0m    Please consider running the darwin/arm64 binary instead.                                              [31m‚îÇ[0m
[31m‚îÇ[0m    Download at https://github.com/kubernetes/minikube/releases/download/v1.26.1/minikube-darwin-arm64    [31m‚îÇ[0m
[31m‚îÇ[0m                                                                                                          [31m‚îÇ[0m
[31m‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ[0m
I0219 15:12:47.464326   39506 root.go:333] Updating PATH: /Users/mikexie/.minikube/bin
I0219 15:12:47.465064   39506 out.go:303] Setting JSON to false
I0219 15:12:47.504653   39506 start.go:115] hostinfo: {"hostname":"Mikes-MBP.lan","uptime":8546897,"bootTime":1668243870,"procs":499,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.2","kernelVersion":"21.3.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"dba72b4f-1677-5036-bfda-458b76b6aa28"}
W0219 15:12:47.504788   39506 start.go:123] gopshost.Virtualization returned error: not implemented yet
I0219 15:12:47.525213   39506 out.go:177] üòÑ  minikube v1.26.1 on Darwin 12.2
I0219 15:12:47.565314   39506 notify.go:193] Checking for updates...
I0219 15:12:47.565797   39506 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0219 15:12:47.585107   39506 out.go:177] üÜï  Kubernetes 1.24.3 is now available. If you would like to upgrade, specify: --kubernetes-version=v1.24.3
I0219 15:12:47.604368   39506 driver.go:365] Setting default libvirt URI to qemu:///system
I0219 15:12:47.738249   39506 docker.go:137] docker version: linux-20.10.12
I0219 15:12:47.738403   39506 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0219 15:12:47.911704   39506 info.go:265] docker info: {ID:CTCL:4RAO:P5RY:ARFY:URFT:O6BG:J3YV:MFMN:UKU5:PBGR:6MIS:ZCYV Containers:4 ContainersRunning:1 ContainersPaused:0 ContainersStopped:3 Images:27 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:79 OomKillDisable:false NGoroutines:68 SystemTime:2023-02-19 07:12:47.838805427 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:2085158912 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0219 15:12:47.932024   39506 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0219 15:12:47.951139   39506 start.go:284] selected driver: docker
I0219 15:12:47.951150   39506 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:1988 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[AmbassadorOperator:datawire/ambassador-operator:v1.2.3@sha256:492f33e0828a371aa23331d75c11c251b21499e31287f026269e3f6ec6da34ed IngressController:ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8 KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0219 15:12:47.951271   39506 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0219 15:12:47.951371   39506 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0219 15:12:48.126041   39506 info.go:265] docker info: {ID:CTCL:4RAO:P5RY:ARFY:URFT:O6BG:J3YV:MFMN:UKU5:PBGR:6MIS:ZCYV Containers:4 ContainersRunning:1 ContainersPaused:0 ContainersStopped:3 Images:27 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:79 OomKillDisable:false NGoroutines:68 SystemTime:2023-02-19 07:12:48.05373301 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:4 KernelVersion:5.10.76-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:5 MemTotal:2085158912 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:20.10.12 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:7b11cfaabd73bb80907dd23182b9347b4245eb5d Expected:7b11cfaabd73bb80907dd23182b9347b4245eb5d} RuncCommit:{ID:v1.0.2-0-g52b36a2 Expected:v1.0.2-0-g52b36a2} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.7.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.2.3] map[Name:scan Path:/usr/local/lib/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I0219 15:12:48.134702   39506 cni.go:95] Creating CNI manager for ""
I0219 15:12:48.134721   39506 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0219 15:12:48.134738   39506 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:1988 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[AmbassadorOperator:datawire/ambassador-operator:v1.2.3@sha256:492f33e0828a371aa23331d75c11c251b21499e31287f026269e3f6ec6da34ed IngressController:ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8 KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0219 15:12:48.175031   39506 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0219 15:12:48.193569   39506 cache.go:120] Beginning downloading kic base image for docker with docker
I0219 15:12:48.212956   39506 out.go:177] üöú  Pulling base image ...
I0219 15:12:48.250123   39506 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0219 15:12:48.250172   39506 image.go:75] Checking for docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon
I0219 15:12:48.250205   39506 preload.go:148] Found local preload: /Users/mikexie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.3-docker-overlay2-arm64.tar.lz4
I0219 15:12:48.250219   39506 cache.go:57] Caching tarball of preloaded images
I0219 15:12:48.255048   39506 preload.go:174] Found /Users/mikexie/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.23.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0219 15:12:48.255218   39506 cache.go:60] Finished verifying existence of preloaded tar for  v1.23.3 on docker
I0219 15:12:48.255576   39506 profile.go:148] Saving config to /Users/mikexie/.minikube/profiles/minikube/config.json ...
I0219 15:12:48.385650   39506 cache.go:147] Downloading docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 to local cache
I0219 15:12:48.385865   39506 image.go:59] Checking for docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local cache directory
I0219 15:12:48.385878   39506 image.go:62] Found docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local cache directory, skipping pull
I0219 15:12:48.385882   39506 image.go:103] docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 exists in cache, skipping pull
I0219 15:12:48.385895   39506 cache.go:150] successfully saved docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 as a tarball
I0219 15:12:48.386004   39506 cache.go:161] Loading docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 from local cache
I0219 15:12:59.393889   39506 cache.go:164] successfully loaded docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 from cached tarball
I0219 15:12:59.393919   39506 cache.go:170] Downloading docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 to local daemon
I0219 15:12:59.394325   39506 image.go:75] Checking for docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 in local docker daemon
I0219 15:12:59.521140   39506 image.go:243] Writing docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 to local daemon
I0219 15:16:01.194391   39506 cache.go:173] successfully downloaded docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2
I0219 15:16:01.195272   39506 cache.go:208] Successfully downloaded all kic artifacts
I0219 15:16:01.195503   39506 start.go:371] acquiring machines lock for minikube: {Name:mk56a3fab771d4090d4990143ce7160693ad7b8f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0219 15:16:01.195851   39506 start.go:375] acquired machines lock for "minikube" in 305.625¬µs
I0219 15:16:01.196047   39506 start.go:95] Skipping create...Using existing machine configuration
I0219 15:16:01.196060   39506 fix.go:55] fixHost starting: 
I0219 15:16:01.197194   39506 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 15:16:01.331260   39506 fix.go:103] recreateIfNeeded on minikube: state=Running err=<nil>
W0219 15:16:01.331302   39506 fix.go:129] unexpected machine state, will restart: <nil>
I0219 15:16:01.353830   39506 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0219 15:16:01.392159   39506 machine.go:88] provisioning docker machine ...
I0219 15:16:01.392201   39506 ubuntu.go:169] provisioning hostname "minikube"
I0219 15:16:01.392307   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:01.528350   39506 main.go:134] libmachine: Using SSH client type: native
I0219 15:16:01.528852   39506 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 53006 <nil> <nil>}
I0219 15:16:01.528943   39506 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0219 15:16:01.710406   39506 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0219 15:16:01.710758   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:01.827278   39506 main.go:134] libmachine: Using SSH client type: native
I0219 15:16:01.827491   39506 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 53006 <nil> <nil>}
I0219 15:16:01.827504   39506 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0219 15:16:01.954154   39506 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0219 15:16:01.954475   39506 ubuntu.go:175] set auth options {CertDir:/Users/mikexie/.minikube CaCertPath:/Users/mikexie/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/mikexie/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/mikexie/.minikube/machines/server.pem ServerKeyPath:/Users/mikexie/.minikube/machines/server-key.pem ClientKeyPath:/Users/mikexie/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/mikexie/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/mikexie/.minikube}
I0219 15:16:01.954504   39506 ubuntu.go:177] setting up certificates
I0219 15:16:01.954530   39506 provision.go:83] configureAuth start
I0219 15:16:01.954623   39506 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0219 15:16:02.070389   39506 provision.go:138] copyHostCerts
I0219 15:16:02.070500   39506 exec_runner.go:144] found /Users/mikexie/.minikube/ca.pem, removing ...
I0219 15:16:02.070507   39506 exec_runner.go:207] rm: /Users/mikexie/.minikube/ca.pem
I0219 15:16:02.071584   39506 exec_runner.go:151] cp: /Users/mikexie/.minikube/certs/ca.pem --> /Users/mikexie/.minikube/ca.pem (1078 bytes)
I0219 15:16:02.072795   39506 exec_runner.go:144] found /Users/mikexie/.minikube/cert.pem, removing ...
I0219 15:16:02.072798   39506 exec_runner.go:207] rm: /Users/mikexie/.minikube/cert.pem
I0219 15:16:02.072852   39506 exec_runner.go:151] cp: /Users/mikexie/.minikube/certs/cert.pem --> /Users/mikexie/.minikube/cert.pem (1123 bytes)
I0219 15:16:02.073125   39506 exec_runner.go:144] found /Users/mikexie/.minikube/key.pem, removing ...
I0219 15:16:02.073127   39506 exec_runner.go:207] rm: /Users/mikexie/.minikube/key.pem
I0219 15:16:02.073204   39506 exec_runner.go:151] cp: /Users/mikexie/.minikube/certs/key.pem --> /Users/mikexie/.minikube/key.pem (1675 bytes)
I0219 15:16:02.073379   39506 provision.go:112] generating server cert: /Users/mikexie/.minikube/machines/server.pem ca-key=/Users/mikexie/.minikube/certs/ca.pem private-key=/Users/mikexie/.minikube/certs/ca-key.pem org=mikexie.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0219 15:16:02.463395   39506 provision.go:172] copyRemoteCerts
I0219 15:16:02.463616   39506 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0219 15:16:02.463658   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:02.581814   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:02.676691   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0219 15:16:02.696705   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I0219 15:16:02.710409   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0219 15:16:02.726044   39506 provision.go:86] duration metric: configureAuth took 771.495792ms
I0219 15:16:02.726149   39506 ubuntu.go:193] setting minikube options for container-runtime
I0219 15:16:02.726535   39506 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0219 15:16:02.726614   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:02.845511   39506 main.go:134] libmachine: Using SSH client type: native
I0219 15:16:02.845721   39506 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 53006 <nil> <nil>}
I0219 15:16:02.845732   39506 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0219 15:16:02.970674   39506 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I0219 15:16:02.971043   39506 ubuntu.go:71] root file system type: overlay
I0219 15:16:02.971370   39506 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0219 15:16:02.971463   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:03.086412   39506 main.go:134] libmachine: Using SSH client type: native
I0219 15:16:03.086592   39506 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 53006 <nil> <nil>}
I0219 15:16:03.086644   39506 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0219 15:16:03.221774   39506 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0219 15:16:03.222236   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:03.344499   39506 main.go:134] libmachine: Using SSH client type: native
I0219 15:16:03.344681   39506 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x13d2e20] 0x13d5e80 <nil>  [] 0s} 127.0.0.1 53006 <nil> <nil>}
I0219 15:16:03.344692   39506 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0219 15:16:03.477532   39506 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0219 15:16:03.477877   39506 machine.go:91] provisioned docker machine in 2.085688375s
I0219 15:16:03.477954   39506 start.go:307] post-start starting for "minikube" (driver="docker")
I0219 15:16:03.477958   39506 start.go:335] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0219 15:16:03.478061   39506 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0219 15:16:03.478106   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:03.596174   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:03.687297   39506 ssh_runner.go:195] Run: cat /etc/os-release
I0219 15:16:03.691880   39506 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0219 15:16:03.691900   39506 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0219 15:16:03.691906   39506 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0219 15:16:03.691917   39506 info.go:137] Remote host: Ubuntu 20.04.2 LTS
I0219 15:16:03.691942   39506 filesync.go:126] Scanning /Users/mikexie/.minikube/addons for local assets ...
I0219 15:16:03.692127   39506 filesync.go:126] Scanning /Users/mikexie/.minikube/files for local assets ...
I0219 15:16:03.692174   39506 start.go:310] post-start completed in 214.214666ms
I0219 15:16:03.692265   39506 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0219 15:16:03.692312   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:03.811107   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:03.897698   39506 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0219 15:16:03.902087   39506 fix.go:57] fixHost completed within 2.705999916s
I0219 15:16:03.902124   39506 start.go:82] releasing machines lock for "minikube", held for 2.706076083s
I0219 15:16:03.902436   39506 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0219 15:16:04.022440   39506 ssh_runner.go:195] Run: systemctl --version
I0219 15:16:04.022507   39506 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0219 15:16:04.022524   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:04.026735   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:04.148141   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:04.148241   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:04.236947   39506 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0219 15:16:06.275591   39506 ssh_runner.go:235] Completed: curl -sS -m 2 https://k8s.gcr.io/: (2.252883208s)
W0219 15:16:06.275846   39506 start.go:734] [curl -sS -m 2 https://k8s.gcr.io/] failed: curl -sS -m 2 https://k8s.gcr.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Connection timed out after 2002 milliseconds
I0219 15:16:06.276133   39506 ssh_runner.go:235] Completed: sudo systemctl cat docker.service: (2.039127458s)
W0219 15:16:06.276681   39506 out.go:239] ‚ùó  This container is having trouble accessing https://k8s.gcr.io
I0219 15:16:06.277027   39506 cruntime.go:273] skipping containerd shutdown because we are bound to it
I0219 15:16:06.277731   39506 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
W0219 15:16:06.278118   39506 out.go:239] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0219 15:16:06.291492   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/dockershim.sock
image-endpoint: unix:///var/run/dockershim.sock
" | sudo tee /etc/crictl.yaml"
I0219 15:16:06.303734   39506 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0219 15:16:06.437652   39506 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0219 15:16:06.546433   39506 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0219 15:16:06.651916   39506 ssh_runner.go:195] Run: sudo systemctl restart docker
I0219 15:16:27.909096   39506 ssh_runner.go:235] Completed: sudo systemctl restart docker: (21.256942792s)
I0219 15:16:27.910725   39506 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0219 15:16:28.108900   39506 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0219 15:16:28.238327   39506 out.go:204] üê≥  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
I0219 15:16:28.239432   39506 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0219 15:16:28.521640   39506 network.go:96] got host ip for mount in container by digging dns: 192.168.65.2
I0219 15:16:28.522434   39506 ssh_runner.go:195] Run: grep 192.168.65.2	host.minikube.internal$ /etc/hosts
I0219 15:16:28.570823   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0219 15:16:28.719956   39506 out.go:177]     ‚ñ™ kubelet.housekeeping-interval=5m
I0219 15:16:28.739387   39506 preload.go:132] Checking if preload exists for k8s version v1.23.3 and runtime docker
I0219 15:16:28.739503   39506 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0219 15:16:28.813708   39506 docker.go:611] Got preloaded images: -- stdout --
moshu/auth:latest
<none>:<none>
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
quay.io/datawire/ambassador-operator:<none>

-- /stdout --
I0219 15:16:28.813889   39506 docker.go:542] Images already preloaded, skipping extraction
I0219 15:16:28.814055   39506 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0219 15:16:28.891519   39506 docker.go:611] Got preloaded images: -- stdout --
moshu/auth:latest
<none>:<none>
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/kube-apiserver:v1.23.3
k8s.gcr.io/kube-controller-manager:v1.23.3
k8s.gcr.io/kube-proxy:v1.23.3
k8s.gcr.io/kube-scheduler:v1.23.3
k8s.gcr.io/etcd:3.5.1-0
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
quay.io/datawire/ambassador-operator:<none>

-- /stdout --
I0219 15:16:28.891831   39506 cache_images.go:84] Images are preloaded, skipping loading
I0219 15:16:28.891968   39506 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0219 15:16:29.584873   39506 cni.go:95] Creating CNI manager for ""
I0219 15:16:29.584896   39506 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0219 15:16:29.585086   39506 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0219 15:16:29.585781   39506 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.23.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/dockershim.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0219 15:16:29.586028   39506 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.23.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0219 15:16:29.586832   39506 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.23.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=docker --hostname-override=minikube --housekeeping-interval=5m --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0219 15:16:29.587026   39506 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.23.3
I0219 15:16:29.597210   39506 binaries.go:44] Found k8s binaries, skipping transfer
I0219 15:16:29.597373   39506 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0219 15:16:29.606879   39506 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (361 bytes)
I0219 15:16:29.620827   39506 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0219 15:16:29.681283   39506 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2029 bytes)
I0219 15:16:29.693610   39506 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0219 15:16:29.697532   39506 certs.go:54] Setting up /Users/mikexie/.minikube/profiles/minikube for IP: 192.168.49.2
I0219 15:16:29.697732   39506 certs.go:182] skipping minikubeCA CA generation: /Users/mikexie/.minikube/ca.key
I0219 15:16:29.698047   39506 certs.go:182] skipping proxyClientCA CA generation: /Users/mikexie/.minikube/proxy-client-ca.key
I0219 15:16:29.698186   39506 certs.go:298] skipping minikube-user signed cert generation: /Users/mikexie/.minikube/profiles/minikube/client.key
I0219 15:16:29.698400   39506 certs.go:298] skipping minikube signed cert generation: /Users/mikexie/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0219 15:16:29.698656   39506 certs.go:298] skipping aggregator signed cert generation: /Users/mikexie/.minikube/profiles/minikube/proxy-client.key
I0219 15:16:29.699155   39506 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/ca-key.pem (1679 bytes)
I0219 15:16:29.699389   39506 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/ca.pem (1078 bytes)
I0219 15:16:29.699449   39506 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/cert.pem (1123 bytes)
I0219 15:16:29.699603   39506 certs.go:388] found cert: /Users/mikexie/.minikube/certs/Users/mikexie/.minikube/certs/key.pem (1675 bytes)
I0219 15:16:29.700836   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0219 15:16:29.783804   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0219 15:16:29.802336   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0219 15:16:29.869551   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0219 15:16:29.892932   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0219 15:16:29.911682   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0219 15:16:29.986462   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0219 15:16:30.004630   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0219 15:16:30.072475   39506 ssh_runner.go:362] scp /Users/mikexie/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0219 15:16:30.099175   39506 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0219 15:16:30.114018   39506 ssh_runner.go:195] Run: openssl version
I0219 15:16:30.166277   39506 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0219 15:16:30.176374   39506 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0219 15:16:30.180709   39506 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Mar 19  2022 /usr/share/ca-certificates/minikubeCA.pem
I0219 15:16:30.180782   39506 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0219 15:16:30.186893   39506 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0219 15:16:30.194705   39506 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:docker.io/kicbase/stable:v0.0.30@sha256:02c921df998f95e849058af14de7045efc3954d90320967418a0d1f182bbc0b2 Memory:1988 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.23.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:kubelet Key:housekeeping-interval Value:5m}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[AmbassadorOperator:datawire/ambassador-operator:v1.2.3@sha256:492f33e0828a371aa23331d75c11c251b21499e31287f026269e3f6ec6da34ed IngressController:ingress-nginx/controller:v1.2.1@sha256:5516d103a9c2ecc4f026efbd4b40662ce22dc1f824fb129ed121460aaa5c47f8 KubeWebhookCertgenCreate:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660 KubeWebhookCertgenPatch:k8s.gcr.io/ingress-nginx/kube-webhook-certgen:v1.1.1@sha256:64d8c73dca984af206adf9d6d7e46aa550362b1d7a01f3a0a91b20cc67868660] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0219 15:16:30.194947   39506 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0219 15:16:30.225664   39506 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0219 15:16:30.273967   39506 kubeadm.go:410] found existing configuration files, will attempt cluster restart
I0219 15:16:30.273991   39506 kubeadm.go:626] restartCluster start
I0219 15:16:30.274104   39506 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0219 15:16:30.281199   39506 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0219 15:16:30.281826   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0219 15:16:30.412070   39506 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:53005"
I0219 15:16:30.413231   39506 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0219 15:16:30.422517   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:30.422603   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:30.436487   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:30.638859   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:30.639188   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:30.651483   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:30.838001   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:30.839773   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:30.851545   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:31.037406   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:31.038701   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:31.049312   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:31.237785   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:31.238761   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:31.250572   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:31.437792   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:31.438784   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:31.450846   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:31.637810   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:31.638715   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:31.653103   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:31.838307   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:31.838730   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:31.854883   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:32.037739   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:32.038293   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:32.051933   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:32.237618   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:32.237823   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:32.249591   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:32.438102   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:32.449440   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:32.461824   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:32.637482   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:32.637919   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:32.651781   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:32.839587   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:32.839902   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:32.852462   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:33.038551   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:33.038809   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:33.049835   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:33.238582   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:33.238851   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:33.255621   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:33.437348   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:33.438302   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:33.453490   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:33.453505   39506 api_server.go:165] Checking apiserver status ...
I0219 15:16:33.453563   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0219 15:16:33.466378   39506 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0219 15:16:33.466931   39506 kubeadm.go:601] needs reconfigure: apiserver error: timed out waiting for the condition
I0219 15:16:33.466983   39506 kubeadm.go:1092] stopping kube-system containers ...
I0219 15:16:33.467084   39506 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0219 15:16:33.499980   39506 docker.go:443] Stopping containers: [13e35fa1e944 728c89cf69fb 6afcf831171f 1bbe28728211 f853b0b98ab5 39fb63082d68 d75f1c94fcee 5c1934c1d3bb 9b42b4165e70 de1875b56731 f7ef0895ed89 7bc5abcbaefe 264af5fe4f69 9cbdea118533 e74c1d6da2e1 547855e93acc 2d1cb67deb40 5f2c39076c9d 853fcf9aeb0f 7b33377dee7c 84a8f0ad14fe 4003e6eb5c2b 7696f00b44c2 d5e2ca9af092 b80490cf367b 32d9c5e120cd e965c7bcf302 f7aab14f76ff 40455bae70ee 5eb9d85e54e2 2898bd0335c9 9eb1cd10609b 563da215ec2b 466598daadd6 59ec07ba6046 d34184192ea5 87f6d32146dd 9f1615926455 cd01db85af73 06aa03008080 3fc36816360a 1b29d00fbb15 f32102795210 d2f79697b585]
I0219 15:16:33.500191   39506 ssh_runner.go:195] Run: docker stop 13e35fa1e944 728c89cf69fb 6afcf831171f 1bbe28728211 f853b0b98ab5 39fb63082d68 d75f1c94fcee 5c1934c1d3bb 9b42b4165e70 de1875b56731 f7ef0895ed89 7bc5abcbaefe 264af5fe4f69 9cbdea118533 e74c1d6da2e1 547855e93acc 2d1cb67deb40 5f2c39076c9d 853fcf9aeb0f 7b33377dee7c 84a8f0ad14fe 4003e6eb5c2b 7696f00b44c2 d5e2ca9af092 b80490cf367b 32d9c5e120cd e965c7bcf302 f7aab14f76ff 40455bae70ee 5eb9d85e54e2 2898bd0335c9 9eb1cd10609b 563da215ec2b 466598daadd6 59ec07ba6046 d34184192ea5 87f6d32146dd 9f1615926455 cd01db85af73 06aa03008080 3fc36816360a 1b29d00fbb15 f32102795210 d2f79697b585
I0219 15:16:42.253637   39506 ssh_runner.go:235] Completed: docker stop 13e35fa1e944 728c89cf69fb 6afcf831171f 1bbe28728211 f853b0b98ab5 39fb63082d68 d75f1c94fcee 5c1934c1d3bb 9b42b4165e70 de1875b56731 f7ef0895ed89 7bc5abcbaefe 264af5fe4f69 9cbdea118533 e74c1d6da2e1 547855e93acc 2d1cb67deb40 5f2c39076c9d 853fcf9aeb0f 7b33377dee7c 84a8f0ad14fe 4003e6eb5c2b 7696f00b44c2 d5e2ca9af092 b80490cf367b 32d9c5e120cd e965c7bcf302 f7aab14f76ff 40455bae70ee 5eb9d85e54e2 2898bd0335c9 9eb1cd10609b 563da215ec2b 466598daadd6 59ec07ba6046 d34184192ea5 87f6d32146dd 9f1615926455 cd01db85af73 06aa03008080 3fc36816360a 1b29d00fbb15 f32102795210 d2f79697b585: (8.753275875s)
I0219 15:16:42.254232   39506 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0219 15:16:42.295773   39506 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0219 15:16:42.303516   39506 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Feb 19 07:11 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Feb 19 07:11 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Feb 19 07:11 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Feb 19 07:11 /etc/kubernetes/scheduler.conf

I0219 15:16:42.303718   39506 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0219 15:16:42.311351   39506 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0219 15:16:42.318512   39506 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0219 15:16:42.324428   39506 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0219 15:16:42.324492   39506 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0219 15:16:42.331375   39506 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0219 15:16:42.337401   39506 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0219 15:16:42.337470   39506 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0219 15:16:42.344036   39506 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0219 15:16:42.352102   39506 kubeadm.go:703] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0219 15:16:42.352111   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0219 15:16:42.451930   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0219 15:16:42.945223   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0219 15:16:43.086168   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0219 15:16:43.132254   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0219 15:16:43.174820   39506 api_server.go:51] waiting for apiserver process to appear ...
I0219 15:16:43.174952   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0219 15:16:43.687689   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0219 15:16:44.187810   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0219 15:16:44.270463   39506 api_server.go:71] duration metric: took 1.095635542s to wait for apiserver process to appear ...
I0219 15:16:44.270545   39506 api_server.go:87] waiting for apiserver healthz status ...
I0219 15:16:44.270564   39506 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:53005/healthz ...
I0219 15:16:46.775893   39506 api_server.go:266] https://127.0.0.1:53005/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0219 15:16:46.776027   39506 api_server.go:102] status: https://127.0.0.1:53005/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0219 15:16:47.277285   39506 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:53005/healthz ...
I0219 15:16:47.288739   39506 api_server.go:266] https://127.0.0.1:53005/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0219 15:16:47.288756   39506 api_server.go:102] status: https://127.0.0.1:53005/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0219 15:16:47.777856   39506 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:53005/healthz ...
I0219 15:16:47.790136   39506 api_server.go:266] https://127.0.0.1:53005/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
W0219 15:16:47.790178   39506 api_server.go:102] status: https://127.0.0.1:53005/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
healthz check failed
I0219 15:16:48.277631   39506 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:53005/healthz ...
I0219 15:16:48.287413   39506 api_server.go:266] https://127.0.0.1:53005/healthz returned 200:
ok
I0219 15:16:48.302770   39506 api_server.go:140] control plane version: v1.23.3
I0219 15:16:48.302870   39506 api_server.go:130] duration metric: took 4.032281083s to wait for apiserver health ...
I0219 15:16:48.302904   39506 cni.go:95] Creating CNI manager for ""
I0219 15:16:48.302908   39506 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0219 15:16:48.302931   39506 system_pods.go:43] waiting for kube-system pods to appear ...
I0219 15:16:48.312112   39506 system_pods.go:59] 7 kube-system pods found
I0219 15:16:48.312186   39506 system_pods.go:61] "coredns-64897985d-vl5tw" [231354c8-5ab1-4a58-b2e0-5ba62fe203f5] Running
I0219 15:16:48.312189   39506 system_pods.go:61] "etcd-minikube" [97a6127a-e219-483d-a26a-b91c397579f5] Running
I0219 15:16:48.312192   39506 system_pods.go:61] "kube-apiserver-minikube" [65dde80e-728d-4306-b093-1059d06762a6] Running
I0219 15:16:48.312194   39506 system_pods.go:61] "kube-controller-manager-minikube" [1d73e422-39f7-4d02-8ed7-60f5ffd17138] Running
I0219 15:16:48.312197   39506 system_pods.go:61] "kube-proxy-g9p5l" [12e1a0cb-15b8-4e64-b304-524565e67d2f] Running
I0219 15:16:48.312199   39506 system_pods.go:61] "kube-scheduler-minikube" [1a7fd0af-c051-455d-9c47-9c5a3763d5c0] Running
I0219 15:16:48.312201   39506 system_pods.go:61] "storage-provisioner" [825f9f8b-a349-4846-999c-33bae928cd31] Running
I0219 15:16:48.312306   39506 system_pods.go:74] duration metric: took 9.371208ms to wait for pod list to return data ...
I0219 15:16:48.312331   39506 node_conditions.go:102] verifying NodePressure condition ...
I0219 15:16:48.318726   39506 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0219 15:16:48.318741   39506 node_conditions.go:123] node cpu capacity is 5
I0219 15:16:48.318757   39506 node_conditions.go:105] duration metric: took 6.423083ms to run NodePressure ...
I0219 15:16:48.318787   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.23.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0219 15:16:48.456740   39506 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0219 15:16:48.468717   39506 ops.go:34] apiserver oom_adj: -16
I0219 15:16:48.468767   39506 kubeadm.go:630] restartCluster took 18.194607333s
I0219 15:16:48.468843   39506 kubeadm.go:397] StartCluster complete in 18.273988125s
I0219 15:16:48.468956   39506 settings.go:142] acquiring lock: {Name:mk71fc66d07302884b478819f412a69d5f194e24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 15:16:48.469705   39506 settings.go:150] Updating kubeconfig:  /Users/mikexie/.kube/config
I0219 15:16:48.471930   39506 lock.go:35] WriteFile acquiring /Users/mikexie/.kube/config: {Name:mk6bfba26889efd951ba6b3d878ab2d33a30dc0f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0219 15:16:48.477032   39506 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0219 15:16:48.477306   39506 start.go:211] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.23.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0219 15:16:48.479019   39506 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.23.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0219 15:16:48.498444   39506 out.go:177] üîé  Verifying Kubernetes components...
I0219 15:16:48.477740   39506 addons.go:412] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0219 15:16:48.481088   39506 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.23.3
I0219 15:16:48.554387   39506 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0219 15:16:48.554611   39506 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0219 15:16:48.554621   39506 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0219 15:16:48.554614   39506 addons.go:65] Setting dashboard=true in profile "minikube"
I0219 15:16:48.555021   39506 addons.go:153] Setting addon dashboard=true in "minikube"
W0219 15:16:48.555029   39506 addons.go:162] addon dashboard should already be in state true
I0219 15:16:48.555035   39506 addons.go:153] Setting addon storage-provisioner=true in "minikube"
W0219 15:16:48.555046   39506 addons.go:162] addon storage-provisioner should already be in state true
I0219 15:16:48.555065   39506 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0219 15:16:48.555797   39506 host.go:66] Checking if "minikube" exists ...
I0219 15:16:48.555904   39506 host.go:66] Checking if "minikube" exists ...
I0219 15:16:48.556119   39506 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 15:16:48.556171   39506 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 15:16:48.568696   39506 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 15:16:48.688104   39506 start.go:789] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0219 15:16:48.688770   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0219 15:16:48.734181   39506 addons.go:153] Setting addon default-storageclass=true in "minikube"
I0219 15:16:48.745563   39506 out.go:177]     ‚ñ™ Using image kubernetesui/dashboard:v2.6.0
I0219 15:16:48.745854   39506 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W0219 15:16:48.746191   39506 addons.go:162] addon default-storageclass should already be in state true
I0219 15:16:48.780867   39506 host.go:66] Checking if "minikube" exists ...
I0219 15:16:48.818276   39506 out.go:177]     ‚ñ™ Using image kubernetesui/metrics-scraper:v1.0.8
I0219 15:16:48.799344   39506 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0219 15:16:48.799531   39506 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0219 15:16:48.837473   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0219 15:16:48.837484   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0219 15:16:48.837543   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:48.842409   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0219 15:16:48.842680   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:48.860465   39506 api_server.go:51] waiting for apiserver process to appear ...
I0219 15:16:48.860581   39506 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0219 15:16:48.874620   39506 api_server.go:71] duration metric: took 397.186209ms to wait for apiserver process to appear ...
I0219 15:16:48.874743   39506 api_server.go:87] waiting for apiserver healthz status ...
I0219 15:16:48.874769   39506 api_server.go:240] Checking apiserver healthz at https://127.0.0.1:53005/healthz ...
I0219 15:16:48.884222   39506 api_server.go:266] https://127.0.0.1:53005/healthz returned 200:
ok
I0219 15:16:48.886304   39506 api_server.go:140] control plane version: v1.23.3
I0219 15:16:48.886316   39506 api_server.go:130] duration metric: took 11.569417ms to wait for apiserver health ...
I0219 15:16:48.886441   39506 system_pods.go:43] waiting for kube-system pods to appear ...
I0219 15:16:48.895905   39506 system_pods.go:59] 7 kube-system pods found
I0219 15:16:48.895920   39506 system_pods.go:61] "coredns-64897985d-vl5tw" [231354c8-5ab1-4a58-b2e0-5ba62fe203f5] Running
I0219 15:16:48.895923   39506 system_pods.go:61] "etcd-minikube" [97a6127a-e219-483d-a26a-b91c397579f5] Running
I0219 15:16:48.895925   39506 system_pods.go:61] "kube-apiserver-minikube" [65dde80e-728d-4306-b093-1059d06762a6] Running
I0219 15:16:48.895928   39506 system_pods.go:61] "kube-controller-manager-minikube" [1d73e422-39f7-4d02-8ed7-60f5ffd17138] Running
I0219 15:16:48.895930   39506 system_pods.go:61] "kube-proxy-g9p5l" [12e1a0cb-15b8-4e64-b304-524565e67d2f] Running
I0219 15:16:48.895932   39506 system_pods.go:61] "kube-scheduler-minikube" [1a7fd0af-c051-455d-9c47-9c5a3763d5c0] Running
I0219 15:16:48.895934   39506 system_pods.go:61] "storage-provisioner" [825f9f8b-a349-4846-999c-33bae928cd31] Running
I0219 15:16:48.895939   39506 system_pods.go:74] duration metric: took 9.491833ms to wait for pod list to return data ...
I0219 15:16:48.896037   39506 kubeadm.go:572] duration metric: took 418.61275ms to wait for : map[apiserver:true system_pods:true] ...
I0219 15:16:48.896051   39506 node_conditions.go:102] verifying NodePressure condition ...
I0219 15:16:48.905892   39506 node_conditions.go:122] node storage ephemeral capacity is 61255492Ki
I0219 15:16:48.905903   39506 node_conditions.go:123] node cpu capacity is 5
I0219 15:16:48.905913   39506 node_conditions.go:105] duration metric: took 9.859375ms to run NodePressure ...
I0219 15:16:48.906047   39506 start.go:216] waiting for startup goroutines ...
I0219 15:16:48.967989   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:48.968228   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:48.971431   39506 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I0219 15:16:48.971439   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0219 15:16:48.971504   39506 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0219 15:16:49.071182   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0219 15:16:49.071199   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0219 15:16:49.071818   39506 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0219 15:16:49.085015   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0219 15:16:49.085036   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0219 15:16:49.090669   39506 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53006 SSHKeyPath:/Users/mikexie/.minikube/machines/minikube/id_rsa Username:docker}
I0219 15:16:49.097666   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0219 15:16:49.097677   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0219 15:16:49.111250   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0219 15:16:49.111261   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4278 bytes)
I0219 15:16:49.125217   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-role.yaml
I0219 15:16:49.125230   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0219 15:16:49.136682   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0219 15:16:49.136690   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0219 15:16:49.168349   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0219 15:16:49.168362   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0219 15:16:49.184173   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0219 15:16:49.184184   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0219 15:16:49.187797   39506 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0219 15:16:49.195879   39506 addons.go:345] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0219 15:16:49.195886   39506 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0219 15:16:49.210847   39506 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.23.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0219 15:16:49.733420   39506 out.go:177] üåü  Enabled addons: dashboard, default-storageclass, storage-provisioner
I0219 15:16:49.771604   39506 addons.go:414] enableAddons completed in 1.293890292s
I0219 15:16:49.809442   39506 start.go:506] kubectl: 1.22.5, cluster: 1.23.3 (minor skew: 1)
I0219 15:16:49.829272   39506 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Sun 2023-02-19 07:11:33 UTC, end at Sun 2023-02-19 07:32:39 UTC. --
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.982445885Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.982455885Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.984614427Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.984631677Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.984639885Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.984645177Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Feb 19 07:16:16 minikube dockerd[5411]: time="2023-02-19T07:16:16.988705177Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.029443093Z" level=info msg="Loading containers: start."
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.169841968Z" level=info msg="ignoring event" container=547855e93acc27dfc56ed21c5ff7991c27bce5721843ccf6dc20be56b207456f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.169870718Z" level=info msg="ignoring event" container=2d1cb67deb406dcaca71dffd7b02bd0ae285dbcf14eded8c2f0c3465fbdf9d12 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.169882093Z" level=info msg="ignoring event" container=84a8f0ad14fe12dea68d950919c8b1f9b5d6ed89cea3710991881b9570a90fae module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.172085552Z" level=info msg="ignoring event" container=7b33377dee7cc015e59f817de867a8f5f01aec1b818a1a78b5aa2197f9f8d0fd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.173912343Z" level=info msg="ignoring event" container=e74c1d6da2e13ce8ee229042034e78ff5991e4e6eda229ff474c24fa3fabea79 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.174488635Z" level=info msg="ignoring event" container=9cbdea11853341c68fffe6d87164d3e20b221ac83c5de265a3035a7c4ab694b0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:17 minikube dockerd[5411]: time="2023-02-19T07:16:17.178266510Z" level=info msg="ignoring event" container=264af5fe4f697de8ecdd1d7995526a66789394bf42d5c4b6d87cdb906e79bcf1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.092424542Z" level=info msg="Container failed to exit within 10s of signal 15 - using the force" container=5f2c39076c9dc0b425d559f705c244271fb33085e95a55c9bdeaf5fe50001b65
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.113064584Z" level=info msg="Container failed to exit within 10s of signal 15 - using the force" container=853fcf9aeb0f4a495a04cd19d57d256ea1da87bdff84c0aea437222b42b24a4a
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.170375500Z" level=info msg="ignoring event" container=5f2c39076c9dc0b425d559f705c244271fb33085e95a55c9bdeaf5fe50001b65 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.172149959Z" level=info msg="ignoring event" container=853fcf9aeb0f4a495a04cd19d57d256ea1da87bdff84c0aea437222b42b24a4a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.302967209Z" level=info msg="Removing stale sandbox e5b78d119b9bb9098b8cbb3d5b59fd883eb50e32e3c9a696071b92be2ffbee76 (9cbdea11853341c68fffe6d87164d3e20b221ac83c5de265a3035a7c4ab694b0)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.304363834Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 7920a29fa4bec90b463daad5c49ed5de7fa882c34464d4ee6fe295a36e059e80 f373f7455ad320bc39177bdf8f286e77d9dd9256568dd3a8a89b416c932adf85], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.376397667Z" level=info msg="Removing stale sandbox 43e9bc47a76bad4555dc3cf6f174a35cb01446ffc457f2e604e046a2d2e6b222 (2d1cb67deb406dcaca71dffd7b02bd0ae285dbcf14eded8c2f0c3465fbdf9d12)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.378378459Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 7920a29fa4bec90b463daad5c49ed5de7fa882c34464d4ee6fe295a36e059e80 10931294c76ad9a36a36ec5861ce70fda57f169827a447d37aa95e1ce7cca7d9], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.451469542Z" level=info msg="Removing stale sandbox 55337f7bb803542bbe25f5c1de6db8c359928105a679e3831e67af18787441ca (7b33377dee7cc015e59f817de867a8f5f01aec1b818a1a78b5aa2197f9f8d0fd)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.452997542Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 7920a29fa4bec90b463daad5c49ed5de7fa882c34464d4ee6fe295a36e059e80 db17af226213e1ccb880d76064b2333d0c0b3cd9f41bad45b135d6eb47151854], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.523296584Z" level=info msg="Removing stale sandbox 598772cc99792e21344c5a8189a666ce3815060ac12f0eff264d36e6352aaec6 (84a8f0ad14fe12dea68d950919c8b1f9b5d6ed89cea3710991881b9570a90fae)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.524321792Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 7920a29fa4bec90b463daad5c49ed5de7fa882c34464d4ee6fe295a36e059e80 01c48c92f25c1ab39427a3b5e88d3d2ed5f13d5419e55c82299a94e9a73f0ba7], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.598713751Z" level=info msg="Removing stale sandbox 620dd46295a305b3e8973f5f37be2f75f3633f21fe91782dc4ac921842bb071b (264af5fe4f697de8ecdd1d7995526a66789394bf42d5c4b6d87cdb906e79bcf1)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.601598584Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 712ff43105efa60a7e422f6f8621574108019c98db9796033e8b89f592fd768b 79aab8175d1541b46dae88136aee8e916b836dac0a0c9a6c543649f9a183871c], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.680007751Z" level=info msg="Removing stale sandbox 6466da8f81df44dca09d2e5ed6a53f5fc92410db87da7afb291f58c8774859f9 (547855e93acc27dfc56ed21c5ff7991c27bce5721843ccf6dc20be56b207456f)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.681207792Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 7920a29fa4bec90b463daad5c49ed5de7fa882c34464d4ee6fe295a36e059e80 67660a7b10b532b7d29d4640aecf51a18ff9b21d365d364fac9cb0260661b725], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.752787334Z" level=info msg="Removing stale sandbox bb39de2f3106cf4d7d955da537c4333c8a5d8b9e3a5d470ab245b5de8ed82d15 (e74c1d6da2e13ce8ee229042034e78ff5991e4e6eda229ff474c24fa3fabea79)"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.754433792Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 7920a29fa4bec90b463daad5c49ed5de7fa882c34464d4ee6fe295a36e059e80 763a50fb726ecbdcd9f013d70f4474091c4043cb51c1431a359f5d5283c8f18e], retrying...."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.773706334Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.822605292Z" level=info msg="Loading containers: done."
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.834787876Z" level=info msg="Docker daemon" commit=459d0df graphdriver(s)=overlay2 version=20.10.12
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.834855751Z" level=info msg="Daemon has completed initialization"
Feb 19 07:16:27 minikube systemd[1]: Started Docker Application Container Engine.
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.904363251Z" level=info msg="API listen on [::]:2376"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.905126834Z" level=info msg="API listen on /var/run/docker.sock"
Feb 19 07:16:27 minikube dockerd[5411]: time="2023-02-19T07:16:27.997674709Z" level=error msg="7bc5abcbaefe450506e682a2c1fbc853fbf452f0a4f1f1682e6e02b0fe6c6924 cleanup: failed to delete container from containerd: no such container"
Feb 19 07:16:29 minikube dockerd[5411]: time="2023-02-19T07:16:29.375895210Z" level=info msg="ignoring event" container=007c3882530701c11014694cdf61ed67df5b2a375cc4d02e404786caa174ef4d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.565389545Z" level=info msg="ignoring event" container=728c89cf69fbf1ef9b8dfb42abef022a03cf4cabb52900ca7e65e4212cdbcf45 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.565459378Z" level=info msg="ignoring event" container=13e35fa1e944a83e71dd7d6bb5d496c582e0df2e81b9b1c2262332d40ddf27dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.573503212Z" level=info msg="ignoring event" container=de1875b567318c5c8582fbed295b35b869141cd944f9505d48e48ac1419d563b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.574487503Z" level=info msg="ignoring event" container=f7ef0895ed89ceff8924a275c9024b2cb96fc6b927ba7d47d145d1f2f513911e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.579133587Z" level=info msg="ignoring event" container=9b42b4165e709c56028f35b0dbc1ba397cf9a18978f7bd7ef74b7d1e23e5f9c1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.595431545Z" level=info msg="ignoring event" container=d75f1c94fceece17cc822d1ea997d19997c2814baa7e2503218e10e427959123 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.596133337Z" level=info msg="ignoring event" container=1bbe287282113d43eb6ef5c4d94cf178a39b871fce955cc22e91ee5c55f63bff module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.598849087Z" level=info msg="ignoring event" container=5c1934c1d3bb2819d1f02478dd105564f659841fee771d9165c30e2a0c30edc5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.603059587Z" level=info msg="ignoring event" container=39fb63082d680d2bfcc4c6a0b65d8e30e2d1899bb5e43a005d73cd6faee6f6d1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:33 minikube dockerd[5411]: time="2023-02-19T07:16:33.667345129Z" level=info msg="ignoring event" container=f853b0b98ab5abbe5bacc3e6c1ab6eb2bb611f77b6f999bee52ef77a38aab49f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:42 minikube dockerd[5411]: time="2023-02-19T07:16:42.230402591Z" level=info msg="ignoring event" container=6afcf831171f4c548ff54ef25f0d7ab02ad180c5ad0b67e8a1c15ead923e3d66 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:16:52 minikube dockerd[5411]: time="2023-02-19T07:16:52.553521263Z" level=info msg="ignoring event" container=423856506d33efbd1cf38e1f658298e18604f8cd6816856579e2892d2bb74cf3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:17:09 minikube dockerd[5411]: time="2023-02-19T07:17:09.947413631Z" level=info msg="ignoring event" container=465003e5bda540bbe454b7b669d9078523cff15061c2e77b34ee4b1273054aef module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:17:38 minikube dockerd[5411]: time="2023-02-19T07:17:38.992371631Z" level=info msg="ignoring event" container=00339fbf6f10f4a695482c70581a8c613730ba8dac077844121cefe00e74a638 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:18:32 minikube dockerd[5411]: time="2023-02-19T07:18:32.931949336Z" level=info msg="ignoring event" container=f7f9f4598be487d75ac5980a9c1baf7c69017349a30f4d53575bd9dcfd645683 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:20:05 minikube dockerd[5411]: time="2023-02-19T07:20:05.923519171Z" level=info msg="ignoring event" container=ce77d6838f485530a8e3d91d658f64ed6ed78c846ec04173cdf1aad7c5f740e8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:23:00 minikube dockerd[5411]: time="2023-02-19T07:23:00.969867210Z" level=info msg="ignoring event" container=9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 19 07:28:13 minikube dockerd[5411]: time="2023-02-19T07:28:13.966725008Z" level=info msg="ignoring event" container=1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID
1a7d630f6b319       a4fefcde6b458                                                                                          4 minutes ago       Exited              ambassador-operator         33                  3321afd18cc2a
0f9d2ae96631d       09df423d5a37f                                                                                          15 minutes ago      Running             kubernetes-dashboard        7                   2dfb7ca09b9ca
521ed5f408ec3       d36a89daa1945                                                                                          15 minutes ago      Running             kube-proxy                  5                   20801d41a62a1
7b4285f05bf77       edaa71f2aee88                                                                                          15 minutes ago      Running             coredns                     5                   b1971cd7de183
f5ab8b1f23205       ba04bb24b9575                                                                                          15 minutes ago      Running             storage-provisioner         7                   a53771f08560f
c16b801e9236a       3e63a2140741e                                                                                          15 minutes ago      Running             kube-controller-manager     12                  275799fea0ce8
90d3169955b24       8e7422f73cf36                                                                                          15 minutes ago      Running             kube-apiserver              12                  12c3cea89ab1c
5bb9779652f44       1040f7790951c                                                                                          15 minutes ago      Running             etcd                        12                  84403d0bddeb6
4b0d92ad3f1e6       4bad79a8953b4                                                                                          15 minutes ago      Running             kube-scheduler              12                  e6b690a76fb26
79be99156b2c7       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     16 minutes ago      Running             auth                        4                   ad1b31127aab4
2b6894b084335       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     16 minutes ago      Running             auth                        4                   b0a7666c0c30b
728c89cf69fbf       1040f7790951c                                                                                          16 minutes ago      Exited              etcd                        11                  1bbe287282113
e3deddccbd6f2       a422e0e982356                                                                                          16 minutes ago      Running             dashboard-metrics-scraper   4                   1eb7942929782
007c388253070       09df423d5a37f                                                                                          16 minutes ago      Exited              kubernetes-dashboard        6                   2dfb7ca09b9ca
6afcf831171f4       edaa71f2aee88                                                                                          16 minutes ago      Exited              coredns                     4                   5c1934c1d3bb2
39fb63082d680       3e63a2140741e                                                                                          16 minutes ago      Exited              kube-controller-manager     11                  f7ef0895ed89c
d75f1c94fceec       d36a89daa1945                                                                                          16 minutes ago      Exited              kube-proxy                  4                   9b42b4165e709
7bc5abcbaefe4       ba04bb24b9575                                                                                          16 minutes ago      Created             storage-provisioner         6                   547855e93acc2
5f2c39076c9dc       8e7422f73cf36                                                                                          16 minutes ago      Exited              kube-apiserver              11                  7b33377dee7cc
853fcf9aeb0f4       4bad79a8953b4                                                                                          16 minutes ago      Exited              kube-scheduler              11                  84a8f0ad14fe1
4b9b459f122a2       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     20 minutes ago      Exited              auth                        3                   2a296030141cf
d5de2b25b5ec0       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   20 minutes ago      Exited              dashboard-metrics-scraper   3                   76541e04c1c5f
07d28f0a1c7da       moshu/auth@sha256:c9ed4bd2fc29e0988c09481c78f7e16846772cf90533c7d57ec75b067435899e                     20 minutes ago      Exited              auth                        3                   26c71666218db

* 
* ==> coredns [6afcf831171f] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/arm64, go1.17.1, 13a9191
[INFO] plugin/health: Going into lameduck mode for 5s
[ERROR] plugin/errors: 2 283744096059473130.3646197341201287563. HINFO: dial udp 192.168.65.2:53: connect: network is unreachable
[ERROR] plugin/errors: 2 283744096059473130.3646197341201287563. HINFO: dial udp 192.168.65.2:53: connect: network is unreachable

* 
* ==> coredns [7b4285f05bf7] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = c23ed519c17e71ee396ed052e6209e94
CoreDNS-1.8.6
linux/arm64, go1.17.1, 13a9191

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              <none>
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 12 Feb 2023 10:29:26 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 19 Feb 2023 07:32:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 19 Feb 2023 07:32:07 +0000   Sun, 12 Feb 2023 10:29:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 19 Feb 2023 07:32:07 +0000   Sun, 12 Feb 2023 10:29:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 19 Feb 2023 07:32:07 +0000   Sun, 12 Feb 2023 10:29:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 19 Feb 2023 07:32:07 +0000   Sun, 12 Feb 2023 10:29:36 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                5
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2036288Ki
  pods:               110
Allocatable:
  cpu:                5
  ephemeral-storage:  61255492Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             2036288Ki
  pods:               110
System Info:
  Machine ID:                 7f42765e713c4b909dde4d5f15b8d18f
  System UUID:                7f42765e713c4b909dde4d5f15b8d18f
  Boot ID:                    291575da-373f-4327-9afd-35a2ed4851f4
  Kernel Version:             5.10.76-linuxkit
  OS Image:                   Ubuntu 20.04.2 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://20.10.12
  Kubelet Version:            v1.23.3
  Kube-Proxy Version:         v1.23.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ambassador                  ambassador-operator-5dc77dccd7-wm7mc         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h10m
  default                     auth-7c5dbfc47d-pdm75                        500m (10%!)(MISSING)    500m (10%!)(MISSING)  128Mi (6%!)(MISSING)       128Mi (6%!)(MISSING)     6d21h
  default                     auth-7ffb89668c-fbntt                        500m (10%!)(MISSING)    500m (10%!)(MISSING)  1Gi (51%!)(MISSING)        1Gi (51%!)(MISSING)      6d21h
  kube-system                 coredns-64897985d-vl5tw                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (3%!)(MISSING)        170Mi (8%!)(MISSING)     6d21h
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (5%!)(MISSING)       0 (0%!)(MISSING)         6d21h
  kube-system                 kube-apiserver-minikube                      250m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
  kube-system                 kube-controller-manager-minikube             200m (4%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
  kube-system                 kube-proxy-g9p5l                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
  kubernetes-dashboard        dashboard-metrics-scraper-65b4bd797-wv5c4    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
  kubernetes-dashboard        kubernetes-dashboard-cd7c84bfc-dgw9r         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         6d21h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1750m (35%!)(MISSING)   1 (20%!)(MISSING)
  memory             1322Mi (66%!)(MISSING)  1322Mi (66%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)        0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)        0 (0%!)(MISSING)
Events:
  Type     Reason                                            Age                From        Message
  ----     ------                                            ----               ----        -------
  Warning  listen tcp4 :30244: bind: address already in use  15m                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:30244/tcp4), skipping it
  Normal   Starting                                          20m                kube-proxy  
  Warning  listen tcp4 :32210: bind: address already in use  15m                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:32210/tcp4), skipping it
  Warning  listen tcp4 :32210: bind: address already in use  20m                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:32210/tcp4), skipping it
  Normal   Starting                                          15m                kube-proxy  
  Normal   Starting                                          76m                kube-proxy  
  Warning  listen tcp4 :30509: bind: address already in use  76m                kube-proxy  can't open port "nodePort for default/hello-minikube" (:30509/tcp4), skipping it
  Warning  listen tcp4 :30244: bind: address already in use  76m                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:30244/tcp4), skipping it
  Warning  listen tcp4 :32210: bind: address already in use  76m                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:https" (:32210/tcp4), skipping it
  Warning  listen tcp4 :30509: bind: address already in use  15m                kube-proxy  can't open port "nodePort for default/hello-minikube" (:30509/tcp4), skipping it
  Warning  listen tcp4 :30244: bind: address already in use  20m                kube-proxy  can't open port "nodePort for ingress-nginx/ingress-nginx-controller:http" (:30244/tcp4), skipping it
  Warning  listen tcp4 :30509: bind: address already in use  20m                kube-proxy  can't open port "nodePort for default/hello-minikube" (:30509/tcp4), skipping it
  Normal   NodeAllocatableEnforced                           76m                kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientPID                              76m (x7 over 76m)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure                             76m (x8 over 76m)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientMemory                           76m (x8 over 76m)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                                          76m                kubelet     Starting kubelet.
  Normal   NodeHasSufficientPID                              20m (x7 over 20m)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory                           20m (x8 over 20m)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                                          20m                kubelet     Starting kubelet.
  Normal   NodeAllocatableEnforced                           20m                kubelet     Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure                             20m (x8 over 20m)  kubelet     Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID                              15m (x7 over 15m)  kubelet     Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced                           15m                kubelet     Updated Node Allocatable limit across pods
  Normal   Starting                                          15m                kubelet     Starting kubelet.
  Normal   NodeHasSufficientMemory                           15m (x8 over 15m)  kubelet     Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure                             15m (x8 over 15m)  kubelet     Node minikube status is now: NodeHasNoDiskPressure

* 
* ==> dmesg <==
* [Feb 6 23:28] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.027987] the cryptoloop driver has been deprecated and will be removed in in Linux 5.16
[  +6.004956] grpcfuse: loading out-of-tree module taints kernel.
[Feb 6 23:37] hrtimer: interrupt took 4337541 ns

* 
* ==> etcd [5bb9779652f4] <==
* {"level":"warn","ts":1676791004.1052978,"caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-02-19T07:16:44.106Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-02-19T07:16:44.106Z","caller":"etcdmain/etcd.go:115","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-02-19T07:16:44.106Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-02-19T07:16:44.106Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-02-19T07:16:44.106Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-02-19T07:16:44.106Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.1","git-sha":"d42e8589e","go-version":"go1.16.2","go-os":"linux","go-arch":"arm64","max-cpu-set":5,"max-cpu-available":5,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-02-19T07:16:44.109Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.272792ms"}
{"level":"info","ts":"2023-02-19T07:16:45.117Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":680068,"snapshot-size":"10 kB"}
{"level":"info","ts":"2023-02-19T07:16:45.117Z","caller":"etcdserver/server.go:518","msg":"recovered v3 backend from snapshot","backend-size-bytes":6598656,"backend-size":"6.6 MB","backend-size-in-use-bytes":4620288,"backend-size-in-use":"4.6 MB"}
{"level":"info","ts":"2023-02-19T07:16:45.267Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":681934}
{"level":"info","ts":"2023-02-19T07:16:45.268Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-02-19T07:16:45.268Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 13"}
{"level":"info","ts":"2023-02-19T07:16:45.268Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 13, commit: 681934, applied: 680068, lastindex: 681934, lastterm: 13]"}
{"level":"info","ts":"2023-02-19T07:16:45.268Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-02-19T07:16:45.268Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-02-19T07:16:45.268Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-02-19T07:16:45.269Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-02-19T07:16:45.270Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":527642}
{"level":"info","ts":"2023-02-19T07:16:45.282Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":528430}
{"level":"info","ts":"2023-02-19T07:16:45.282Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-02-19T07:16:45.283Z","caller":"etcdserver/server.go:834","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.1","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-02-19T07:16:45.283Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-02-19T07:16:45.285Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-02-19T07:16:45.285Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:45.285Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:45.285Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-02-19T07:16:45.285Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 13"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 13"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 13"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 14"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 14"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 14"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 14"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-02-19T07:16:45.669Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-02-19T07:16:45.672Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-02-19T07:16:45.672Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-02-19T07:26:45.696Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":528820}
{"level":"info","ts":"2023-02-19T07:26:45.737Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":528820,"took":"39.8625ms"}
{"level":"info","ts":"2023-02-19T07:31:45.701Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":529044}
{"level":"info","ts":"2023-02-19T07:31:45.702Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":529044,"took":"663.083¬µs"}

* 
* ==> etcd [728c89cf69fb] <==
* {"level":"warn","ts":1676790989.5025098,"caller":"flags/flag.go:93","msg":"unrecognized environment variable","environment-variable":"ETCD_UNSUPPORTED_ARCH=arm64"}
{"level":"info","ts":"2023-02-19T07:16:29.502Z","caller":"etcdmain/etcd.go:72","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2023-02-19T07:16:29.502Z","caller":"etcdmain/etcd.go:115","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2023-02-19T07:16:29.502Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-02-19T07:16:29.503Z","caller":"embed/etcd.go:478","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-02-19T07:16:29.503Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2023-02-19T07:16:29.503Z","caller":"embed/etcd.go:307","msg":"starting an etcd server","etcd-version":"3.5.1","git-sha":"d42e8589e","go-version":"go1.16.2","go-os":"linux","go-arch":"arm64","max-cpu-set":5,"max-cpu-available":5,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":false,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2023-02-19T07:16:29.565Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"61.683416ms"}
{"level":"info","ts":"2023-02-19T07:16:30.642Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":680068,"snapshot-size":"10 kB"}
{"level":"info","ts":"2023-02-19T07:16:30.642Z","caller":"etcdserver/server.go:518","msg":"recovered v3 backend from snapshot","backend-size-bytes":6598656,"backend-size":"6.6 MB","backend-size-in-use-bytes":4620288,"backend-size-in-use":"4.6 MB"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":681932}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 12"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 12, commit: 681932, applied: 680068, lastindex: 681932, lastterm: 12]"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2023-02-19T07:16:30.726Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2023-02-19T07:16:30.728Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2023-02-19T07:16:30.729Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":527642}
{"level":"info","ts":"2023-02-19T07:16:30.730Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":528430}
{"level":"info","ts":"2023-02-19T07:16:30.731Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2023-02-19T07:16:30.731Z","caller":"etcdserver/server.go:834","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.1","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2023-02-19T07:16:30.732Z","caller":"etcdserver/server.go:728","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:580","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:552","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:276","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2023-02-19T07:16:30.734Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 12"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 12"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 12"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 13"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"etcdserver/server.go:2027","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"etcdmain/main.go:47","msg":"notifying init daemon"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"etcdmain/main.go:53","msg":"successfully notified init daemon"}
{"level":"info","ts":"2023-02-19T07:16:31.631Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2023-02-19T07:16:31.634Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2023-02-19T07:16:31.635Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2023-02-19T07:16:33.532Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-02-19T07:16:33.532Z","caller":"embed/etcd.go:367","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
WARNING: 2023/02/19 07:16:33 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2023/02/19 07:16:33 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2023-02-19T07:16:33.535Z","caller":"etcdserver/server.go:1438","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-02-19T07:16:33.537Z","caller":"embed/etcd.go:562","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:33.538Z","caller":"embed/etcd.go:567","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-02-19T07:16:33.538Z","caller":"embed/etcd.go:369","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  07:32:40 up 12 days,  8:04,  0 users,  load average: 0.27, 0.30, 0.33
Linux minikube 5.10.76-linuxkit #1 SMP PREEMPT Mon Nov 8 11:22:26 UTC 2021 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.2 LTS"

* 
* ==> kube-apiserver [5f2c39076c9d] <==
* I0219 07:16:07.704602       1 server.go:565] external host was not specified, using 192.168.49.2
I0219 07:16:07.705080       1 server.go:172] Version: v1.23.3
I0219 07:16:08.003635       1 shared_informer.go:240] Waiting for caches to sync for node_authorizer
I0219 07:16:08.004993       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0219 07:16:08.005008       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
I0219 07:16:08.005481       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0219 07:16:08.005495       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0219 07:16:08.008029       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:09.004666       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:09.008306       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:10.005617       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:10.650134       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:11.705844       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:13.175314       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:14.309459       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:17.171073       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:18.582999       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:23.775138       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
W0219 07:16:24.133153       1 clientconn.go:1331] [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...

* 
* ==> kube-apiserver [90d3169955b2] <==
* W0219 07:16:46.120507       1 genericapiserver.go:538] Skipping API node.k8s.io/v1alpha1 because it has no resources.
W0219 07:16:46.124613       1 genericapiserver.go:538] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0219 07:16:46.124633       1 genericapiserver.go:538] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
W0219 07:16:46.125703       1 genericapiserver.go:538] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0219 07:16:46.125723       1 genericapiserver.go:538] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
W0219 07:16:46.128268       1 genericapiserver.go:538] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
W0219 07:16:46.130894       1 genericapiserver.go:538] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W0219 07:16:46.133548       1 genericapiserver.go:538] Skipping API apps/v1beta2 because it has no resources.
W0219 07:16:46.133562       1 genericapiserver.go:538] Skipping API apps/v1beta1 because it has no resources.
W0219 07:16:46.136941       1 genericapiserver.go:538] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0219 07:16:46.142441       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0219 07:16:46.142460       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0219 07:16:46.158362       1 genericapiserver.go:538] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0219 07:16:46.745925       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0219 07:16:46.745961       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0219 07:16:46.746290       1 dynamic_serving_content.go:131] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0219 07:16:46.746349       1 secure_serving.go:266] Serving securely on [::]:8443
I0219 07:16:46.746381       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0219 07:16:46.746874       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0219 07:16:46.747833       1 dynamic_serving_content.go:131] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0219 07:16:46.750543       1 available_controller.go:491] Starting AvailableConditionController
I0219 07:16:46.750557       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0219 07:16:46.750566       1 controller.go:83] Starting OpenAPI AggregationController
I0219 07:16:46.750583       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I0219 07:16:46.751596       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0219 07:16:46.751609       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0219 07:16:46.767464       1 autoregister_controller.go:141] Starting autoregister controller
I0219 07:16:46.767484       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0219 07:16:46.767507       1 controller.go:85] Starting OpenAPI controller
I0219 07:16:46.767516       1 naming_controller.go:291] Starting NamingConditionController
I0219 07:16:46.767525       1 establishing_controller.go:76] Starting EstablishingController
I0219 07:16:46.767536       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0219 07:16:46.767549       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0219 07:16:46.767999       1 crd_finalizer.go:266] Starting CRDFinalizer
I0219 07:16:46.768224       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0219 07:16:46.768473       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0219 07:16:46.768681       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0219 07:16:46.768723       1 shared_informer.go:240] Waiting for caches to sync for cluster_authentication_trust_controller
I0219 07:16:46.768750       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0219 07:16:46.768796       1 shared_informer.go:240] Waiting for caches to sync for crd-autoregister
E0219 07:16:46.779756       1 controller.go:157] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0219 07:16:46.786460       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0219 07:16:46.788632       1 shared_informer.go:247] Caches are synced for node_authorizer 
I0219 07:16:46.859144       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0219 07:16:46.859430       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0219 07:16:46.859678       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0219 07:16:46.867595       1 cache.go:39] Caches are synced for autoregister controller
I0219 07:16:46.869635       1 shared_informer.go:247] Caches are synced for crd-autoregister 
I0219 07:16:46.869650       1 shared_informer.go:247] Caches are synced for cluster_authentication_trust_controller 
I0219 07:16:46.980022       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
I0219 07:16:47.746450       1 controller.go:132] OpenAPI AggregationController: action for item : Nothing (removed from the queue).
I0219 07:16:47.746488       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0219 07:16:47.771718       1 storage_scheduling.go:109] all system priority classes are created successfully or already exist.
I0219 07:16:48.413785       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0219 07:16:48.418007       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0219 07:16:48.434522       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0219 07:16:48.441680       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0219 07:16:48.444598       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0219 07:16:59.917450       1 controller.go:611] quota admission added evaluator for: endpoints
I0219 07:16:59.965039       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io

* 
* ==> kube-controller-manager [39fb63082d68] <==
* I0219 07:16:30.002488       1 serving.go:348] Generated self-signed cert in-memory
I0219 07:16:30.418990       1 controllermanager.go:196] Version: v1.23.3
I0219 07:16:30.421432       1 secure_serving.go:200] Serving securely on 127.0.0.1:10257
I0219 07:16:30.421411       1 dynamic_cafile_content.go:156] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0219 07:16:30.421416       1 dynamic_cafile_content.go:156] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0219 07:16:30.421575       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"

* 
* ==> kube-controller-manager [c16b801e9236] <==
* I0219 07:16:59.662596       1 expand_controller.go:342] Starting expand controller
I0219 07:16:59.662600       1 shared_informer.go:240] Waiting for caches to sync for expand
I0219 07:16:59.713121       1 controllermanager.go:605] Started "pvc-protection"
I0219 07:16:59.713150       1 pvc_protection_controller.go:103] "Starting PVC protection controller"
I0219 07:16:59.713155       1 shared_informer.go:240] Waiting for caches to sync for PVC protection
I0219 07:16:59.764052       1 controllermanager.go:605] Started "ttl-after-finished"
I0219 07:16:59.764064       1 ttlafterfinished_controller.go:109] Starting TTL after finished controller
I0219 07:16:59.764500       1 shared_informer.go:240] Waiting for caches to sync for TTL after finished
I0219 07:16:59.769122       1 shared_informer.go:240] Waiting for caches to sync for resource quota
W0219 07:16:59.773716       1 actual_state_of_world.go:534] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0219 07:16:59.782735       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0219 07:16:59.782763       1 job_controller.go:453] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0219 07:16:59.786461       1 shared_informer.go:240] Waiting for caches to sync for garbage collector
I0219 07:16:59.791552       1 shared_informer.go:247] Caches are synced for ephemeral 
I0219 07:16:59.793767       1 shared_informer.go:247] Caches are synced for endpoint 
I0219 07:16:59.795966       1 shared_informer.go:247] Caches are synced for GC 
I0219 07:16:59.797048       1 shared_informer.go:247] Caches are synced for endpoint_slice 
I0219 07:16:59.802264       1 shared_informer.go:247] Caches are synced for HPA 
I0219 07:16:59.805432       1 shared_informer.go:247] Caches are synced for persistent volume 
I0219 07:16:59.806757       1 shared_informer.go:247] Caches are synced for PV protection 
I0219 07:16:59.813192       1 shared_informer.go:247] Caches are synced for PVC protection 
I0219 07:16:59.813208       1 shared_informer.go:247] Caches are synced for job 
I0219 07:16:59.813459       1 shared_informer.go:247] Caches are synced for service account 
I0219 07:16:59.814528       1 shared_informer.go:247] Caches are synced for taint 
I0219 07:16:59.814574       1 node_lifecycle_controller.go:1397] Initializing eviction metric for zone: 
W0219 07:16:59.814603       1 node_lifecycle_controller.go:1012] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0219 07:16:59.814605       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0219 07:16:59.814617       1 node_lifecycle_controller.go:1213] Controller detected that zone  is now in state Normal.
I0219 07:16:59.814648       1 event.go:294] "Event occurred" object="minikube" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0219 07:16:59.815141       1 shared_informer.go:247] Caches are synced for TTL 
I0219 07:16:59.815186       1 shared_informer.go:247] Caches are synced for namespace 
I0219 07:16:59.819196       1 shared_informer.go:247] Caches are synced for endpoint_slice_mirroring 
I0219 07:16:59.821897       1 shared_informer.go:247] Caches are synced for ClusterRoleAggregator 
I0219 07:16:59.856318       1 shared_informer.go:247] Caches are synced for node 
I0219 07:16:59.856344       1 range_allocator.go:173] Starting range CIDR allocator
I0219 07:16:59.856346       1 shared_informer.go:240] Waiting for caches to sync for cidrallocator
I0219 07:16:59.856350       1 shared_informer.go:247] Caches are synced for cidrallocator 
I0219 07:16:59.861126       1 shared_informer.go:247] Caches are synced for ReplicationController 
I0219 07:16:59.863498       1 shared_informer.go:247] Caches are synced for stateful set 
I0219 07:16:59.863551       1 shared_informer.go:247] Caches are synced for deployment 
I0219 07:16:59.863611       1 shared_informer.go:247] Caches are synced for daemon sets 
I0219 07:16:59.863731       1 shared_informer.go:247] Caches are synced for expand 
I0219 07:16:59.865819       1 shared_informer.go:247] Caches are synced for TTL after finished 
I0219 07:16:59.865909       1 shared_informer.go:247] Caches are synced for ReplicaSet 
I0219 07:16:59.871821       1 shared_informer.go:247] Caches are synced for disruption 
I0219 07:16:59.871840       1 disruption.go:371] Sending events to api server.
I0219 07:16:59.881603       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-client 
I0219 07:16:59.881670       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kubelet-serving 
I0219 07:16:59.881695       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-kube-apiserver-client 
I0219 07:16:59.883765       1 shared_informer.go:247] Caches are synced for certificate-csrsigning-legacy-unknown 
I0219 07:16:59.912925       1 shared_informer.go:247] Caches are synced for certificate-csrapproving 
I0219 07:16:59.969765       1 shared_informer.go:247] Caches are synced for resource quota 
I0219 07:16:59.970610       1 shared_informer.go:247] Caches are synced for resource quota 
I0219 07:16:59.999716       1 shared_informer.go:247] Caches are synced for cronjob 
I0219 07:17:00.063650       1 shared_informer.go:247] Caches are synced for crt configmap 
I0219 07:17:00.063702       1 shared_informer.go:247] Caches are synced for bootstrap_signer 
I0219 07:17:00.090810       1 shared_informer.go:247] Caches are synced for attach detach 
I0219 07:17:00.478250       1 shared_informer.go:247] Caches are synced for garbage collector 
I0219 07:17:00.478301       1 garbagecollector.go:155] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0219 07:17:00.487282       1 shared_informer.go:247] Caches are synced for garbage collector 

* 
* ==> kube-proxy [521ed5f408ec] <==
* I0219 07:16:51.024400       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I0219 07:16:51.024450       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I0219 07:16:51.024471       1 server_others.go:561] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0219 07:16:51.087048       1 server_others.go:206] "Using iptables Proxier"
I0219 07:16:51.087110       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0219 07:16:51.087119       1 server_others.go:214] "Creating dualStackProxier for iptables"
I0219 07:16:51.087160       1 server_others.go:491] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I0219 07:16:51.088375       1 server.go:656] "Version info" version="v1.23.3"
I0219 07:16:51.090166       1 config.go:317] "Starting service config controller"
I0219 07:16:51.090461       1 config.go:226] "Starting endpoint slice config controller"
I0219 07:16:51.090667       1 shared_informer.go:240] Waiting for caches to sync for endpoint slice config
I0219 07:16:51.090875       1 shared_informer.go:240] Waiting for caches to sync for service config
I0219 07:16:51.191116       1 shared_informer.go:247] Caches are synced for service config 
I0219 07:16:51.191118       1 shared_informer.go:247] Caches are synced for endpoint slice config 
E0219 07:16:51.221356       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30244: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:http IP: IPFamily:4 Port:30244 Protocol:TCP}
E0219 07:16:51.221527       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :30509: bind: address already in use" port={Description:nodePort for default/hello-minikube IP: IPFamily:4 Port:30509 Protocol:TCP}
E0219 07:16:51.221642       1 proxier.go:1600] "can't open port, skipping it" err="listen tcp4 :32210: bind: address already in use" port={Description:nodePort for ingress-nginx/ingress-nginx-controller:https IP: IPFamily:4 Port:32210 Protocol:TCP}

* 
* ==> kube-proxy [d75f1c94fcee] <==
* E0219 07:16:28.884935       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0219 07:16:29.957931       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0219 07:16:32.251612       1 node.go:152] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused

* 
* ==> kube-scheduler [4b0d92ad3f1e] <==
* I0219 07:16:45.000471       1 serving.go:348] Generated self-signed cert in-memory
W0219 07:16:46.795475       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0219 07:16:46.796529       1 authentication.go:345] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0219 07:16:46.796914       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0219 07:16:46.796925       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0219 07:16:46.860053       1 server.go:139] "Starting Kubernetes Scheduler" version="v1.23.3"
I0219 07:16:46.873812       1 secure_serving.go:200] Serving securely on 127.0.0.1:10259
I0219 07:16:46.874528       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0219 07:16:46.874090       1 configmap_cafile_content.go:201] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0219 07:16:46.881790       1 shared_informer.go:240] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0219 07:16:46.982265       1 shared_informer.go:247] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file 

* 
* ==> kube-scheduler [853fcf9aeb0f] <==
* I0219 07:16:07.824157       1 serving.go:348] Generated self-signed cert in-memory
W0219 07:16:18.008982       1 authentication.go:345] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": net/http: TLS handshake timeout
W0219 07:16:18.009203       1 authentication.go:346] Continuing without authentication configuration. This may treat all requests as anonymous.
W0219 07:16:18.009225       1 authentication.go:347] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false

* 
* ==> kubelet <==
* -- Logs begin at Sun 2023-02-19 07:11:33 UTC, end at Sun 2023-02-19 07:32:41 UTC. --
Feb 19 07:26:52 minikube kubelet[7635]: E0219 07:26:52.516291    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:27:04 minikube kubelet[7635]: I0219 07:27:04.514984    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:27:04 minikube kubelet[7635]: E0219 07:27:04.515286    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:27:19 minikube kubelet[7635]: I0219 07:27:19.517657    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:27:19 minikube kubelet[7635]: E0219 07:27:19.518515    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:27:33 minikube kubelet[7635]: I0219 07:27:33.517448    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:27:33 minikube kubelet[7635]: E0219 07:27:33.522206    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:27:47 minikube kubelet[7635]: I0219 07:27:47.514923    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:27:47 minikube kubelet[7635]: E0219 07:27:47.515126    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:27:58 minikube kubelet[7635]: I0219 07:27:58.515712    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:27:58 minikube kubelet[7635]: E0219 07:27:58.516336    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:28:11 minikube kubelet[7635]: I0219 07:28:11.515800    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:28:12 minikube kubelet[7635]: I0219 07:28:12.369092    7635 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ambassador/ambassador-operator-5dc77dccd7-wm7mc through plugin: invalid network status for"
Feb 19 07:28:14 minikube kubelet[7635]: I0219 07:28:14.404420    7635 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ambassador/ambassador-operator-5dc77dccd7-wm7mc through plugin: invalid network status for"
Feb 19 07:28:14 minikube kubelet[7635]: I0219 07:28:14.413722    7635 scope.go:110] "RemoveContainer" containerID="9dd4185b90c7e9022d30fd28d75e20dfcc8153bb93c8b02e3040007d5606f2f3"
Feb 19 07:28:14 minikube kubelet[7635]: I0219 07:28:14.414056    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:28:14 minikube kubelet[7635]: E0219 07:28:14.414532    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:28:15 minikube kubelet[7635]: I0219 07:28:15.425035    7635 docker_sandbox.go:402] "Failed to read pod IP from plugin/docker" err="Couldn't find network status for ambassador/ambassador-operator-5dc77dccd7-wm7mc through plugin: invalid network status for"
Feb 19 07:28:26 minikube kubelet[7635]: I0219 07:28:26.516449    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:28:26 minikube kubelet[7635]: E0219 07:28:26.517216    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:28:40 minikube kubelet[7635]: I0219 07:28:40.519620    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:28:40 minikube kubelet[7635]: E0219 07:28:40.523838    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:28:51 minikube kubelet[7635]: I0219 07:28:51.517745    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:28:51 minikube kubelet[7635]: E0219 07:28:51.518142    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:29:02 minikube kubelet[7635]: I0219 07:29:02.514988    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:29:02 minikube kubelet[7635]: E0219 07:29:02.515237    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:29:14 minikube kubelet[7635]: I0219 07:29:14.515971    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:29:14 minikube kubelet[7635]: E0219 07:29:14.516296    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:29:27 minikube kubelet[7635]: I0219 07:29:27.518332    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:29:27 minikube kubelet[7635]: E0219 07:29:27.518570    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:29:38 minikube kubelet[7635]: I0219 07:29:38.520147    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:29:38 minikube kubelet[7635]: E0219 07:29:38.520936    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:29:51 minikube kubelet[7635]: I0219 07:29:51.523324    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:29:51 minikube kubelet[7635]: E0219 07:29:51.525187    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:30:03 minikube kubelet[7635]: I0219 07:30:03.518366    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:30:03 minikube kubelet[7635]: E0219 07:30:03.519243    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:30:18 minikube kubelet[7635]: I0219 07:30:18.517875    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:30:18 minikube kubelet[7635]: E0219 07:30:18.518868    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:30:33 minikube kubelet[7635]: I0219 07:30:33.519843    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:30:33 minikube kubelet[7635]: E0219 07:30:33.522260    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:30:44 minikube kubelet[7635]: I0219 07:30:44.519388    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:30:44 minikube kubelet[7635]: E0219 07:30:44.519698    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:30:56 minikube kubelet[7635]: I0219 07:30:56.517172    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:30:56 minikube kubelet[7635]: E0219 07:30:56.517779    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:31:07 minikube kubelet[7635]: I0219 07:31:07.516859    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:31:07 minikube kubelet[7635]: E0219 07:31:07.517121    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:31:20 minikube kubelet[7635]: I0219 07:31:20.517645    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:31:20 minikube kubelet[7635]: E0219 07:31:20.518150    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:31:33 minikube kubelet[7635]: I0219 07:31:33.516356    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:31:33 minikube kubelet[7635]: E0219 07:31:33.516837    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:31:43 minikube kubelet[7635]: W0219 07:31:43.492800    7635 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Feb 19 07:31:43 minikube kubelet[7635]: W0219 07:31:43.493219    7635 machine.go:65] Cannot read vendor id correctly, set empty.
Feb 19 07:31:46 minikube kubelet[7635]: I0219 07:31:46.517591    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:31:46 minikube kubelet[7635]: E0219 07:31:46.518332    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:32:01 minikube kubelet[7635]: I0219 07:32:01.517429    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:32:01 minikube kubelet[7635]: E0219 07:32:01.517736    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:32:16 minikube kubelet[7635]: I0219 07:32:16.518107    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:32:16 minikube kubelet[7635]: E0219 07:32:16.519645    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467
Feb 19 07:32:31 minikube kubelet[7635]: I0219 07:32:31.521506    7635 scope.go:110] "RemoveContainer" containerID="1a7d630f6b319eccc3b4a1ab02e4f343c8e85cca7a2e6934453adea1f0866193"
Feb 19 07:32:31 minikube kubelet[7635]: E0219 07:32:31.522694    7635 pod_workers.go:918] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ambassador-operator\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=ambassador-operator pod=ambassador-operator-5dc77dccd7-wm7mc_ambassador(bfee5a86-5767-4cf1-b0e0-611f79af4467)\"" pod="ambassador/ambassador-operator-5dc77dccd7-wm7mc" podUID=bfee5a86-5767-4cf1-b0e0-611f79af4467

* 
* ==> kubernetes-dashboard [007c38825307] <==
* 2023/02/19 07:16:29 Using namespace: kubernetes-dashboard
2023/02/19 07:16:29 Using in-cluster config to connect to apiserver
2023/02/19 07:16:29 Using secret token for csrf signing
2023/02/19 07:16:29 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/02/19 07:16:29 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0x40004dfae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x358
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0x40001bd800)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x8c
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x40001bd800)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x40
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:95 +0x1dc

* 
* ==> kubernetes-dashboard [0f9d2ae96631] <==
* 2023/02/19 07:16:51 Using namespace: kubernetes-dashboard
2023/02/19 07:16:51 Using in-cluster config to connect to apiserver
2023/02/19 07:16:51 Using secret token for csrf signing
2023/02/19 07:16:51 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/02/19 07:16:51 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/02/19 07:16:51 Successful initial request to the apiserver, version: v1.23.3
2023/02/19 07:16:51 Generating JWE encryption key
2023/02/19 07:16:51 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/02/19 07:16:51 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/02/19 07:16:51 Initializing JWE encryption key from synchronized object
2023/02/19 07:16:51 Creating in-cluster Sidecar client
2023/02/19 07:16:51 Serving insecurely on HTTP port: 9090
2023/02/19 07:16:51 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/02/19 07:17:21 Successful request to sidecar
2023/02/19 07:16:51 Starting overwatch

* 
* ==> storage-provisioner [7bc5abcbaefe] <==
* 
* 
* ==> storage-provisioner [f5ab8b1f2320] <==
* I0219 07:16:50.207542       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0219 07:16:50.222446       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0219 07:16:50.223061       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0219 07:17:07.675677       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0219 07:17:07.675908       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"5d18e438-eb11-47ce-813c-bda0fe98c604", APIVersion:"v1", ResourceVersion:"528554", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_b4ae6946-efef-432a-86df-32380dbe666b became leader
I0219 07:17:07.676133       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_b4ae6946-efef-432a-86df-32380dbe666b!
I0219 07:17:07.779317       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_b4ae6946-efef-432a-86df-32380dbe666b!

